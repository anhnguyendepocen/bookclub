21/02/15 10:05:49 INFO Master: Started daemon with process name: 17636@DESKTOP-0L62RVG
21/02/15 10:05:50 WARN Shell: Did not find winutils.exe: {}
java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:548)
	at org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:569)
	at org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:78)
	at org.apache.hadoop.conf.Configuration.getBoolean(Configuration.java:1664)
	at org.apache.hadoop.security.SecurityUtil.setConfigurationInternal(SecurityUtil.java:104)
	at org.apache.hadoop.security.SecurityUtil.<clinit>(SecurityUtil.java:88)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:316)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:304)
	at org.apache.hadoop.security.UserGroupInformation.doSubjectLogin(UserGroupInformation.java:1828)
	at org.apache.hadoop.security.UserGroupInformation.createLoginUser(UserGroupInformation.java:710)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:660)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:571)
	at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2412)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2412)
	at org.apache.spark.SecurityManager.<init>(SecurityManager.scala:79)
	at org.apache.spark.deploy.master.Master$.startRpcEnvAndEndpoint(Master.scala:1139)
	at org.apache.spark.deploy.master.Master$.main(Master.scala:1124)
	at org.apache.spark.deploy.master.Master.main(Master.scala)
Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:468)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:439)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:516)
	... 17 more
21/02/15 10:05:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
21/02/15 10:05:50 INFO SecurityManager: Changing view acls to: aaron
21/02/15 10:05:50 INFO SecurityManager: Changing modify acls to: aaron
21/02/15 10:05:50 INFO SecurityManager: Changing view acls groups to: 
21/02/15 10:05:50 INFO SecurityManager: Changing modify acls groups to: 
21/02/15 10:05:50 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(aaron); groups with view permissions: Set(); users  with modify permissions: Set(aaron); groups with modify permissions: Set()
21/02/15 10:05:52 INFO Utils: Successfully started service 'sparkMaster' on port 7077.
21/02/15 10:05:52 INFO Master: Starting Spark master at spark://192.168.0.31:7077
21/02/15 10:05:52 INFO Master: Running Spark version 3.0.1
21/02/15 10:05:52 INFO Utils: Successfully started service 'MasterUI' on port 8080.
21/02/15 10:05:52 INFO MasterWebUI: Bound MasterWebUI to 0.0.0.0, and started at http://host.docker.internal:8080
21/02/15 10:05:53 INFO Master: I have been elected leader! New state: ALIVE
21/02/15 10:07:22 INFO Worker: Started daemon with process name: 7336@DESKTOP-0L62RVG
21/02/15 10:07:22 WARN Shell: Did not find winutils.exe: {}
java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:548)
	at org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:569)
	at org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:78)
	at org.apache.hadoop.conf.Configuration.getBoolean(Configuration.java:1664)
	at org.apache.hadoop.security.SecurityUtil.setConfigurationInternal(SecurityUtil.java:104)
	at org.apache.hadoop.security.SecurityUtil.<clinit>(SecurityUtil.java:88)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:316)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:304)
	at org.apache.hadoop.security.UserGroupInformation.doSubjectLogin(UserGroupInformation.java:1828)
	at org.apache.hadoop.security.UserGroupInformation.createLoginUser(UserGroupInformation.java:710)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:660)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:571)
	at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2412)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2412)
	at org.apache.spark.SecurityManager.<init>(SecurityManager.scala:79)
	at org.apache.spark.deploy.worker.Worker$.startRpcEnvAndEndpoint(Worker.scala:857)
	at org.apache.spark.deploy.worker.Worker$.main(Worker.scala:828)
	at org.apache.spark.deploy.worker.Worker.main(Worker.scala)
Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:468)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:439)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:516)
	... 17 more
21/02/15 10:07:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
21/02/15 10:07:22 INFO SecurityManager: Changing view acls to: aaron
21/02/15 10:07:22 INFO SecurityManager: Changing modify acls to: aaron
21/02/15 10:07:22 INFO SecurityManager: Changing view acls groups to: 
21/02/15 10:07:22 INFO SecurityManager: Changing modify acls groups to: 
21/02/15 10:07:22 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(aaron); groups with view permissions: Set(); users  with modify permissions: Set(aaron); groups with modify permissions: Set()
21/02/15 10:07:24 INFO Utils: Successfully started service 'sparkWorker' on port 65043.
21/02/15 10:07:24 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[main,5,main]
org.apache.spark.SparkException: Invalid master URL: spark://address:port
	at org.apache.spark.util.Utils$.extractHostPortFromSparkUrl(Utils.scala:2397)
	at org.apache.spark.rpc.RpcAddress$.fromSparkURL(RpcAddress.scala:47)
	at org.apache.spark.deploy.worker.Worker$.$anonfun$startRpcEnvAndEndpoint$3(Worker.scala:859)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
	at scala.collection.TraversableLike.map(TraversableLike.scala:238)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)
	at org.apache.spark.deploy.worker.Worker$.startRpcEnvAndEndpoint(Worker.scala:859)
	at org.apache.spark.deploy.worker.Worker$.main(Worker.scala:828)
	at org.apache.spark.deploy.worker.Worker.main(Worker.scala)
21/02/15 10:07:24 INFO ShutdownHookManager: Shutdown hook called
21/02/15 10:08:01 INFO Worker: Started daemon with process name: 7288@DESKTOP-0L62RVG
21/02/15 10:08:01 WARN Shell: Did not find winutils.exe: {}
java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:548)
	at org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:569)
	at org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:78)
	at org.apache.hadoop.conf.Configuration.getBoolean(Configuration.java:1664)
	at org.apache.hadoop.security.SecurityUtil.setConfigurationInternal(SecurityUtil.java:104)
	at org.apache.hadoop.security.SecurityUtil.<clinit>(SecurityUtil.java:88)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:316)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:304)
	at org.apache.hadoop.security.UserGroupInformation.doSubjectLogin(UserGroupInformation.java:1828)
	at org.apache.hadoop.security.UserGroupInformation.createLoginUser(UserGroupInformation.java:710)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:660)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:571)
	at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2412)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2412)
	at org.apache.spark.SecurityManager.<init>(SecurityManager.scala:79)
	at org.apache.spark.deploy.worker.Worker$.startRpcEnvAndEndpoint(Worker.scala:857)
	at org.apache.spark.deploy.worker.Worker$.main(Worker.scala:828)
	at org.apache.spark.deploy.worker.Worker.main(Worker.scala)
Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:468)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:439)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:516)
	... 17 more
21/02/15 10:08:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
21/02/15 10:08:01 INFO SecurityManager: Changing view acls to: aaron
21/02/15 10:08:01 INFO SecurityManager: Changing modify acls to: aaron
21/02/15 10:08:01 INFO SecurityManager: Changing view acls groups to: 
21/02/15 10:08:01 INFO SecurityManager: Changing modify acls groups to: 
21/02/15 10:08:01 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(aaron); groups with view permissions: Set(); users  with modify permissions: Set(aaron); groups with modify permissions: Set()
21/02/15 10:08:03 INFO Utils: Successfully started service 'sparkWorker' on port 65104.
21/02/15 10:08:03 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[main,5,main]
org.apache.spark.SparkException: Invalid master URL: spark://address:port
	at org.apache.spark.util.Utils$.extractHostPortFromSparkUrl(Utils.scala:2397)
	at org.apache.spark.rpc.RpcAddress$.fromSparkURL(RpcAddress.scala:47)
	at org.apache.spark.deploy.worker.Worker$.$anonfun$startRpcEnvAndEndpoint$3(Worker.scala:859)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
	at scala.collection.TraversableLike.map(TraversableLike.scala:238)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)
	at org.apache.spark.deploy.worker.Worker$.startRpcEnvAndEndpoint(Worker.scala:859)
	at org.apache.spark.deploy.worker.Worker$.main(Worker.scala:828)
	at org.apache.spark.deploy.worker.Worker.main(Worker.scala)
21/02/15 10:08:03 INFO ShutdownHookManager: Shutdown hook called
21/02/15 10:09:02 INFO Worker: Started daemon with process name: 8604@DESKTOP-0L62RVG
21/02/15 10:09:03 WARN Shell: Did not find winutils.exe: {}
java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:548)
	at org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:569)
	at org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:78)
	at org.apache.hadoop.conf.Configuration.getBoolean(Configuration.java:1664)
	at org.apache.hadoop.security.SecurityUtil.setConfigurationInternal(SecurityUtil.java:104)
	at org.apache.hadoop.security.SecurityUtil.<clinit>(SecurityUtil.java:88)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:316)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:304)
	at org.apache.hadoop.security.UserGroupInformation.doSubjectLogin(UserGroupInformation.java:1828)
	at org.apache.hadoop.security.UserGroupInformation.createLoginUser(UserGroupInformation.java:710)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:660)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:571)
	at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2412)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2412)
	at org.apache.spark.SecurityManager.<init>(SecurityManager.scala:79)
	at org.apache.spark.deploy.worker.Worker$.startRpcEnvAndEndpoint(Worker.scala:857)
	at org.apache.spark.deploy.worker.Worker$.main(Worker.scala:828)
	at org.apache.spark.deploy.worker.Worker.main(Worker.scala)
Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:468)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:439)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:516)
	... 17 more
21/02/15 10:09:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
21/02/15 10:09:03 INFO SecurityManager: Changing view acls to: aaron
21/02/15 10:09:03 INFO SecurityManager: Changing modify acls to: aaron
21/02/15 10:09:03 INFO SecurityManager: Changing view acls groups to: 
21/02/15 10:09:03 INFO SecurityManager: Changing modify acls groups to: 
21/02/15 10:09:03 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(aaron); groups with view permissions: Set(); users  with modify permissions: Set(aaron); groups with modify permissions: Set()
21/02/15 10:09:05 INFO Utils: Successfully started service 'sparkWorker' on port 65163.
21/02/15 10:09:05 INFO Worker: Starting Spark worker 192.168.0.31:65163 with 12 cores, 14.8 GiB RAM
21/02/15 10:09:05 INFO Worker: Running Spark version 3.0.1
21/02/15 10:09:05 INFO Worker: Spark home: C:\Users\aaron\AppData\Local\spark\SPARK-~1.2\bin\..
21/02/15 10:09:05 INFO ResourceUtils: ==============================================================
21/02/15 10:09:05 INFO ResourceUtils: Resources for spark.worker:

21/02/15 10:09:05 INFO ResourceUtils: ==============================================================
21/02/15 10:09:05 INFO Utils: Successfully started service 'WorkerUI' on port 8081.
21/02/15 10:09:05 INFO WorkerWebUI: Bound WorkerWebUI to 0.0.0.0, and started at http://host.docker.internal:8081
21/02/15 10:09:05 INFO Worker: Connecting to master localhost:8080...
21/02/15 10:09:05 INFO TransportClientFactory: Successfully created connection to localhost/127.0.0.1:8080 after 32 ms (0 ms spent in bootstraps)
21/02/15 10:09:05 WARN TransportChannelHandler: Exception in connection from localhost/127.0.0.1:8080
java.lang.IllegalArgumentException: Too large frame: 5211883372140375593
	at org.sparkproject.guava.base.Preconditions.checkArgument(Preconditions.java:119)
	at org.apache.spark.network.util.TransportFrameDecoder.decodeNext(TransportFrameDecoder.java:148)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:98)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.lang.Thread.run(Unknown Source)
21/02/15 10:09:05 ERROR TransportResponseHandler: Still have 1 requests outstanding when connection from localhost/127.0.0.1:8080 is closed
21/02/15 10:09:05 WARN Worker: Failed to connect to master localhost:8080
org.apache.spark.SparkException: Exception thrown in awaitResult: 
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:302)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
	at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:101)
	at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:109)
	at org.apache.spark.deploy.worker.Worker$$anon$1.run(Worker.scala:277)
	at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
	at java.util.concurrent.FutureTask.run(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
Caused by: java.lang.IllegalArgumentException: Too large frame: 5211883372140375593
	at org.sparkproject.guava.base.Preconditions.checkArgument(Preconditions.java:119)
	at org.apache.spark.network.util.TransportFrameDecoder.decodeNext(TransportFrameDecoder.java:148)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:98)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	... 1 more
21/02/15 10:09:18 INFO Worker: Retrying connection to master (attempt # 1)
21/02/15 10:09:18 INFO Worker: Connecting to master localhost:8080...
21/02/15 10:09:18 INFO TransportClientFactory: Found inactive connection to localhost/127.0.0.1:8080, creating a new one.
21/02/15 10:09:18 INFO TransportClientFactory: Successfully created connection to localhost/127.0.0.1:8080 after 8 ms (0 ms spent in bootstraps)
21/02/15 10:09:18 WARN TransportChannelHandler: Exception in connection from localhost/127.0.0.1:8080
java.lang.IllegalArgumentException: Too large frame: 5211883372140375593
	at org.sparkproject.guava.base.Preconditions.checkArgument(Preconditions.java:119)
	at org.apache.spark.network.util.TransportFrameDecoder.decodeNext(TransportFrameDecoder.java:148)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:98)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.lang.Thread.run(Unknown Source)
21/02/15 10:09:18 ERROR TransportResponseHandler: Still have 1 requests outstanding when connection from localhost/127.0.0.1:8080 is closed
21/02/15 10:09:18 WARN Worker: Failed to connect to master localhost:8080
org.apache.spark.SparkException: Exception thrown in awaitResult: 
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:302)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
	at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:101)
	at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:109)
	at org.apache.spark.deploy.worker.Worker$$anon$1.run(Worker.scala:277)
	at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
	at java.util.concurrent.FutureTask.run(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
Caused by: java.lang.IllegalArgumentException: Too large frame: 5211883372140375593
	at org.sparkproject.guava.base.Preconditions.checkArgument(Preconditions.java:119)
	at org.apache.spark.network.util.TransportFrameDecoder.decodeNext(TransportFrameDecoder.java:148)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:98)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	... 1 more
21/02/15 10:09:31 INFO Worker: Retrying connection to master (attempt # 2)
21/02/15 10:09:31 INFO Worker: Connecting to master localhost:8080...
21/02/15 10:09:31 INFO TransportClientFactory: Found inactive connection to localhost/127.0.0.1:8080, creating a new one.
21/02/15 10:09:31 INFO TransportClientFactory: Successfully created connection to localhost/127.0.0.1:8080 after 4 ms (0 ms spent in bootstraps)
21/02/15 10:09:31 WARN TransportChannelHandler: Exception in connection from localhost/127.0.0.1:8080
java.lang.IllegalArgumentException: Too large frame: 5211883372140375593
	at org.sparkproject.guava.base.Preconditions.checkArgument(Preconditions.java:119)
	at org.apache.spark.network.util.TransportFrameDecoder.decodeNext(TransportFrameDecoder.java:148)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:98)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.lang.Thread.run(Unknown Source)
21/02/15 10:09:31 ERROR TransportResponseHandler: Still have 1 requests outstanding when connection from localhost/127.0.0.1:8080 is closed
21/02/15 10:09:31 WARN Worker: Failed to connect to master localhost:8080
org.apache.spark.SparkException: Exception thrown in awaitResult: 
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:302)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
	at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:101)
	at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:109)
	at org.apache.spark.deploy.worker.Worker$$anon$1.run(Worker.scala:277)
	at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
	at java.util.concurrent.FutureTask.run(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
Caused by: java.lang.IllegalArgumentException: Too large frame: 5211883372140375593
	at org.sparkproject.guava.base.Preconditions.checkArgument(Preconditions.java:119)
	at org.apache.spark.network.util.TransportFrameDecoder.decodeNext(TransportFrameDecoder.java:148)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:98)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	... 1 more
21/02/15 10:09:44 INFO Worker: Retrying connection to master (attempt # 3)
21/02/15 10:09:44 INFO Worker: Connecting to master localhost:8080...
21/02/15 10:09:44 INFO TransportClientFactory: Found inactive connection to localhost/127.0.0.1:8080, creating a new one.
21/02/15 10:09:44 INFO TransportClientFactory: Successfully created connection to localhost/127.0.0.1:8080 after 6 ms (0 ms spent in bootstraps)
21/02/15 10:09:44 WARN TransportChannelHandler: Exception in connection from localhost/127.0.0.1:8080
java.lang.IllegalArgumentException: Too large frame: 5211883372140375593
	at org.sparkproject.guava.base.Preconditions.checkArgument(Preconditions.java:119)
	at org.apache.spark.network.util.TransportFrameDecoder.decodeNext(TransportFrameDecoder.java:148)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:98)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.lang.Thread.run(Unknown Source)
21/02/15 10:09:44 ERROR TransportResponseHandler: Still have 1 requests outstanding when connection from localhost/127.0.0.1:8080 is closed
21/02/15 10:09:44 WARN Worker: Failed to connect to master localhost:8080
org.apache.spark.SparkException: Exception thrown in awaitResult: 
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:302)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
	at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:101)
	at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:109)
	at org.apache.spark.deploy.worker.Worker$$anon$1.run(Worker.scala:277)
	at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
	at java.util.concurrent.FutureTask.run(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
Caused by: java.lang.IllegalArgumentException: Too large frame: 5211883372140375593
	at org.sparkproject.guava.base.Preconditions.checkArgument(Preconditions.java:119)
	at org.apache.spark.network.util.TransportFrameDecoder.decodeNext(TransportFrameDecoder.java:148)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:98)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	... 1 more
21/02/15 10:09:57 INFO Worker: Retrying connection to master (attempt # 4)
21/02/15 10:09:57 INFO Worker: Connecting to master localhost:8080...
21/02/15 10:09:57 INFO TransportClientFactory: Found inactive connection to localhost/127.0.0.1:8080, creating a new one.
21/02/15 10:09:57 INFO TransportClientFactory: Successfully created connection to localhost/127.0.0.1:8080 after 2 ms (0 ms spent in bootstraps)
21/02/15 10:09:57 WARN TransportChannelHandler: Exception in connection from localhost/127.0.0.1:8080
java.lang.IllegalArgumentException: Too large frame: 5211883372140375593
	at org.sparkproject.guava.base.Preconditions.checkArgument(Preconditions.java:119)
	at org.apache.spark.network.util.TransportFrameDecoder.decodeNext(TransportFrameDecoder.java:148)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:98)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.lang.Thread.run(Unknown Source)
21/02/15 10:09:57 ERROR TransportResponseHandler: Still have 1 requests outstanding when connection from localhost/127.0.0.1:8080 is closed
21/02/15 10:09:57 WARN Worker: Failed to connect to master localhost:8080
org.apache.spark.SparkException: Exception thrown in awaitResult: 
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:302)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
	at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:101)
	at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:109)
	at org.apache.spark.deploy.worker.Worker$$anon$1.run(Worker.scala:277)
	at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
	at java.util.concurrent.FutureTask.run(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
Caused by: java.lang.IllegalArgumentException: Too large frame: 5211883372140375593
	at org.sparkproject.guava.base.Preconditions.checkArgument(Preconditions.java:119)
	at org.apache.spark.network.util.TransportFrameDecoder.decodeNext(TransportFrameDecoder.java:148)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:98)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	... 1 more
21/02/15 10:10:08 INFO Worker: Started daemon with process name: 14748@DESKTOP-0L62RVG
21/02/15 10:10:08 WARN Shell: Did not find winutils.exe: {}
java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:548)
	at org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:569)
	at org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:78)
	at org.apache.hadoop.conf.Configuration.getBoolean(Configuration.java:1664)
	at org.apache.hadoop.security.SecurityUtil.setConfigurationInternal(SecurityUtil.java:104)
	at org.apache.hadoop.security.SecurityUtil.<clinit>(SecurityUtil.java:88)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:316)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:304)
	at org.apache.hadoop.security.UserGroupInformation.doSubjectLogin(UserGroupInformation.java:1828)
	at org.apache.hadoop.security.UserGroupInformation.createLoginUser(UserGroupInformation.java:710)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:660)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:571)
	at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2412)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2412)
	at org.apache.spark.SecurityManager.<init>(SecurityManager.scala:79)
	at org.apache.spark.deploy.worker.Worker$.startRpcEnvAndEndpoint(Worker.scala:857)
	at org.apache.spark.deploy.worker.Worker$.main(Worker.scala:828)
	at org.apache.spark.deploy.worker.Worker.main(Worker.scala)
Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:468)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:439)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:516)
	... 17 more
21/02/15 10:10:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
21/02/15 10:10:08 INFO SecurityManager: Changing view acls to: aaron
21/02/15 10:10:08 INFO SecurityManager: Changing modify acls to: aaron
21/02/15 10:10:08 INFO SecurityManager: Changing view acls groups to: 
21/02/15 10:10:08 INFO SecurityManager: Changing modify acls groups to: 
21/02/15 10:10:08 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(aaron); groups with view permissions: Set(); users  with modify permissions: Set(aaron); groups with modify permissions: Set()
21/02/15 10:10:09 INFO Utils: Successfully started service 'sparkWorker' on port 65239.
21/02/15 10:10:10 INFO Worker: Starting Spark worker 192.168.0.31:65239 with 12 cores, 14.8 GiB RAM
21/02/15 10:10:10 INFO Worker: Running Spark version 3.0.1
21/02/15 10:10:10 INFO Worker: Spark home: C:\Users\aaron\AppData\Local\spark\SPARK-~1.2\bin\..
21/02/15 10:10:10 INFO ResourceUtils: ==============================================================
21/02/15 10:10:10 INFO ResourceUtils: Resources for spark.worker:

21/02/15 10:10:10 INFO ResourceUtils: ==============================================================
21/02/15 10:10:10 WARN Utils: Service 'WorkerUI' could not bind on port 8081. Attempting port 8082.
21/02/15 10:10:10 INFO Utils: Successfully started service 'WorkerUI' on port 8082.
21/02/15 10:10:10 INFO WorkerWebUI: Bound WorkerWebUI to 0.0.0.0, and started at http://host.docker.internal:8082
21/02/15 10:10:10 INFO Worker: Connecting to master 192.168.0.31:7077...
21/02/15 10:10:10 INFO TransportClientFactory: Successfully created connection to /192.168.0.31:7077 after 27 ms (0 ms spent in bootstraps)
21/02/15 10:10:10 INFO Master: Registering worker 192.168.0.31:65239 with 12 cores, 14.8 GiB RAM
21/02/15 10:10:10 INFO Worker: Successfully registered with master spark://192.168.0.31:7077
21/02/15 10:10:10 INFO Worker: Retrying connection to master (attempt # 5)
21/02/15 10:10:10 INFO Worker: Connecting to master localhost:8080...
21/02/15 10:10:10 INFO TransportClientFactory: Found inactive connection to localhost/127.0.0.1:8080, creating a new one.
21/02/15 10:10:10 INFO TransportClientFactory: Successfully created connection to localhost/127.0.0.1:8080 after 1 ms (0 ms spent in bootstraps)
21/02/15 10:10:10 WARN TransportChannelHandler: Exception in connection from localhost/127.0.0.1:8080
java.lang.IllegalArgumentException: Too large frame: 5211883372140375593
	at org.sparkproject.guava.base.Preconditions.checkArgument(Preconditions.java:119)
	at org.apache.spark.network.util.TransportFrameDecoder.decodeNext(TransportFrameDecoder.java:148)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:98)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.lang.Thread.run(Unknown Source)
21/02/15 10:10:10 ERROR TransportResponseHandler: Still have 1 requests outstanding when connection from localhost/127.0.0.1:8080 is closed
21/02/15 10:10:10 WARN Worker: Failed to connect to master localhost:8080
org.apache.spark.SparkException: Exception thrown in awaitResult: 
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:302)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
	at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:101)
	at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:109)
	at org.apache.spark.deploy.worker.Worker$$anon$1.run(Worker.scala:277)
	at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
	at java.util.concurrent.FutureTask.run(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
Caused by: java.lang.IllegalArgumentException: Too large frame: 5211883372140375593
	at org.sparkproject.guava.base.Preconditions.checkArgument(Preconditions.java:119)
	at org.apache.spark.network.util.TransportFrameDecoder.decodeNext(TransportFrameDecoder.java:148)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:98)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	... 1 more
21/02/15 10:10:23 INFO Worker: Retrying connection to master (attempt # 6)
21/02/15 10:10:23 INFO Worker: Connecting to master localhost:8080...
21/02/15 10:10:23 INFO TransportClientFactory: Found inactive connection to localhost/127.0.0.1:8080, creating a new one.
21/02/15 10:10:23 INFO TransportClientFactory: Successfully created connection to localhost/127.0.0.1:8080 after 3 ms (0 ms spent in bootstraps)
21/02/15 10:10:23 WARN TransportChannelHandler: Exception in connection from localhost/127.0.0.1:8080
java.lang.IllegalArgumentException: Too large frame: 5211883372140375593
	at org.sparkproject.guava.base.Preconditions.checkArgument(Preconditions.java:119)
	at org.apache.spark.network.util.TransportFrameDecoder.decodeNext(TransportFrameDecoder.java:148)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:98)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.lang.Thread.run(Unknown Source)
21/02/15 10:10:23 ERROR TransportResponseHandler: Still have 1 requests outstanding when connection from localhost/127.0.0.1:8080 is closed
21/02/15 10:10:23 WARN Worker: Failed to connect to master localhost:8080
org.apache.spark.SparkException: Exception thrown in awaitResult: 
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:302)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
	at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:101)
	at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:109)
	at org.apache.spark.deploy.worker.Worker$$anon$1.run(Worker.scala:277)
	at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
	at java.util.concurrent.FutureTask.run(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
Caused by: java.lang.IllegalArgumentException: Too large frame: 5211883372140375593
	at org.sparkproject.guava.base.Preconditions.checkArgument(Preconditions.java:119)
	at org.apache.spark.network.util.TransportFrameDecoder.decodeNext(TransportFrameDecoder.java:148)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:98)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	... 1 more
21/02/15 10:11:38 INFO Worker: Retrying connection to master (attempt # 7)
21/02/15 10:11:38 INFO Worker: Connecting to master localhost:8080...
21/02/15 10:11:38 INFO TransportClientFactory: Found inactive connection to localhost/127.0.0.1:8080, creating a new one.
21/02/15 10:11:38 INFO TransportClientFactory: Successfully created connection to localhost/127.0.0.1:8080 after 1 ms (0 ms spent in bootstraps)
21/02/15 10:11:38 WARN TransportChannelHandler: Exception in connection from localhost/127.0.0.1:8080
java.lang.IllegalArgumentException: Too large frame: 5211883372140375593
	at org.sparkproject.guava.base.Preconditions.checkArgument(Preconditions.java:119)
	at org.apache.spark.network.util.TransportFrameDecoder.decodeNext(TransportFrameDecoder.java:148)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:98)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.lang.Thread.run(Unknown Source)
21/02/15 10:11:38 ERROR TransportResponseHandler: Still have 1 requests outstanding when connection from localhost/127.0.0.1:8080 is closed
21/02/15 10:11:38 WARN Worker: Failed to connect to master localhost:8080
org.apache.spark.SparkException: Exception thrown in awaitResult: 
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:302)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
	at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:101)
	at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:109)
	at org.apache.spark.deploy.worker.Worker$$anon$1.run(Worker.scala:277)
	at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
	at java.util.concurrent.FutureTask.run(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
Caused by: java.lang.IllegalArgumentException: Too large frame: 5211883372140375593
	at org.sparkproject.guava.base.Preconditions.checkArgument(Preconditions.java:119)
	at org.apache.spark.network.util.TransportFrameDecoder.decodeNext(TransportFrameDecoder.java:148)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:98)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	... 1 more
21/02/15 10:12:53 INFO Worker: Retrying connection to master (attempt # 8)
21/02/15 10:12:53 INFO Worker: Connecting to master localhost:8080...
21/02/15 10:12:53 INFO TransportClientFactory: Found inactive connection to localhost/127.0.0.1:8080, creating a new one.
21/02/15 10:12:53 INFO TransportClientFactory: Successfully created connection to localhost/127.0.0.1:8080 after 2 ms (0 ms spent in bootstraps)
21/02/15 10:12:53 WARN TransportChannelHandler: Exception in connection from localhost/127.0.0.1:8080
java.lang.IllegalArgumentException: Too large frame: 5211883372140375593
	at org.sparkproject.guava.base.Preconditions.checkArgument(Preconditions.java:119)
	at org.apache.spark.network.util.TransportFrameDecoder.decodeNext(TransportFrameDecoder.java:148)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:98)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.lang.Thread.run(Unknown Source)
21/02/15 10:12:53 ERROR TransportResponseHandler: Still have 1 requests outstanding when connection from localhost/127.0.0.1:8080 is closed
21/02/15 10:12:53 WARN Worker: Failed to connect to master localhost:8080
org.apache.spark.SparkException: Exception thrown in awaitResult: 
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:302)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
	at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:101)
	at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:109)
	at org.apache.spark.deploy.worker.Worker$$anon$1.run(Worker.scala:277)
	at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
	at java.util.concurrent.FutureTask.run(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
Caused by: java.lang.IllegalArgumentException: Too large frame: 5211883372140375593
	at org.sparkproject.guava.base.Preconditions.checkArgument(Preconditions.java:119)
	at org.apache.spark.network.util.TransportFrameDecoder.decodeNext(TransportFrameDecoder.java:148)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:98)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	... 1 more
21/02/15 10:14:08 INFO Worker: Retrying connection to master (attempt # 9)
21/02/15 10:14:08 INFO Worker: Connecting to master localhost:8080...
21/02/15 10:14:08 INFO TransportClientFactory: Found inactive connection to localhost/127.0.0.1:8080, creating a new one.
21/02/15 10:14:08 INFO TransportClientFactory: Successfully created connection to localhost/127.0.0.1:8080 after 1 ms (0 ms spent in bootstraps)
21/02/15 10:14:08 WARN TransportChannelHandler: Exception in connection from localhost/127.0.0.1:8080
java.lang.IllegalArgumentException: Too large frame: 5211883372140375593
	at org.sparkproject.guava.base.Preconditions.checkArgument(Preconditions.java:119)
	at org.apache.spark.network.util.TransportFrameDecoder.decodeNext(TransportFrameDecoder.java:148)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:98)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.lang.Thread.run(Unknown Source)
21/02/15 10:14:08 ERROR TransportResponseHandler: Still have 1 requests outstanding when connection from localhost/127.0.0.1:8080 is closed
21/02/15 10:14:08 WARN Worker: Failed to connect to master localhost:8080
org.apache.spark.SparkException: Exception thrown in awaitResult: 
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:302)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
	at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:101)
	at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:109)
	at org.apache.spark.deploy.worker.Worker$$anon$1.run(Worker.scala:277)
	at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
	at java.util.concurrent.FutureTask.run(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
Caused by: java.lang.IllegalArgumentException: Too large frame: 5211883372140375593
	at org.sparkproject.guava.base.Preconditions.checkArgument(Preconditions.java:119)
	at org.apache.spark.network.util.TransportFrameDecoder.decodeNext(TransportFrameDecoder.java:148)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:98)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	... 1 more
21/02/15 10:15:23 INFO Worker: Retrying connection to master (attempt # 10)
21/02/15 10:15:23 INFO Worker: Connecting to master localhost:8080...
21/02/15 10:15:23 INFO TransportClientFactory: Found inactive connection to localhost/127.0.0.1:8080, creating a new one.
21/02/15 10:15:23 INFO TransportClientFactory: Successfully created connection to localhost/127.0.0.1:8080 after 6 ms (0 ms spent in bootstraps)
21/02/15 10:15:23 WARN TransportChannelHandler: Exception in connection from localhost/127.0.0.1:8080
java.lang.IllegalArgumentException: Too large frame: 5211883372140375593
	at org.sparkproject.guava.base.Preconditions.checkArgument(Preconditions.java:119)
	at org.apache.spark.network.util.TransportFrameDecoder.decodeNext(TransportFrameDecoder.java:148)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:98)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.lang.Thread.run(Unknown Source)
21/02/15 10:15:23 ERROR TransportResponseHandler: Still have 1 requests outstanding when connection from localhost/127.0.0.1:8080 is closed
21/02/15 10:15:23 WARN Worker: Failed to connect to master localhost:8080
org.apache.spark.SparkException: Exception thrown in awaitResult: 
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:302)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
	at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:101)
	at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:109)
	at org.apache.spark.deploy.worker.Worker$$anon$1.run(Worker.scala:277)
	at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
	at java.util.concurrent.FutureTask.run(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
Caused by: java.lang.IllegalArgumentException: Too large frame: 5211883372140375593
	at org.sparkproject.guava.base.Preconditions.checkArgument(Preconditions.java:119)
	at org.apache.spark.network.util.TransportFrameDecoder.decodeNext(TransportFrameDecoder.java:148)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:98)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	... 1 more
21/02/15 10:16:38 INFO Worker: Retrying connection to master (attempt # 11)
21/02/15 10:16:38 INFO Worker: Connecting to master localhost:8080...
21/02/15 10:16:38 INFO TransportClientFactory: Found inactive connection to localhost/127.0.0.1:8080, creating a new one.
21/02/15 10:16:38 INFO TransportClientFactory: Successfully created connection to localhost/127.0.0.1:8080 after 2 ms (0 ms spent in bootstraps)
21/02/15 10:16:38 WARN TransportChannelHandler: Exception in connection from localhost/127.0.0.1:8080
java.lang.IllegalArgumentException: Too large frame: 5211883372140375593
	at org.sparkproject.guava.base.Preconditions.checkArgument(Preconditions.java:119)
	at org.apache.spark.network.util.TransportFrameDecoder.decodeNext(TransportFrameDecoder.java:148)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:98)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.lang.Thread.run(Unknown Source)
21/02/15 10:16:38 ERROR TransportResponseHandler: Still have 1 requests outstanding when connection from localhost/127.0.0.1:8080 is closed
21/02/15 10:16:38 WARN Worker: Failed to connect to master localhost:8080
org.apache.spark.SparkException: Exception thrown in awaitResult: 
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:302)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
	at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:101)
	at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:109)
	at org.apache.spark.deploy.worker.Worker$$anon$1.run(Worker.scala:277)
	at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
	at java.util.concurrent.FutureTask.run(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
Caused by: java.lang.IllegalArgumentException: Too large frame: 5211883372140375593
	at org.sparkproject.guava.base.Preconditions.checkArgument(Preconditions.java:119)
	at org.apache.spark.network.util.TransportFrameDecoder.decodeNext(TransportFrameDecoder.java:148)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:98)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	... 1 more
21/02/15 10:17:53 INFO Worker: Retrying connection to master (attempt # 12)
21/02/15 10:17:53 INFO Worker: Connecting to master localhost:8080...
21/02/15 10:17:53 INFO TransportClientFactory: Found inactive connection to localhost/127.0.0.1:8080, creating a new one.
21/02/15 10:17:53 INFO TransportClientFactory: Successfully created connection to localhost/127.0.0.1:8080 after 5 ms (0 ms spent in bootstraps)
21/02/15 10:17:53 WARN TransportChannelHandler: Exception in connection from localhost/127.0.0.1:8080
java.lang.IllegalArgumentException: Too large frame: 5211883372140375593
	at org.sparkproject.guava.base.Preconditions.checkArgument(Preconditions.java:119)
	at org.apache.spark.network.util.TransportFrameDecoder.decodeNext(TransportFrameDecoder.java:148)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:98)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.lang.Thread.run(Unknown Source)
21/02/15 10:17:53 ERROR TransportResponseHandler: Still have 1 requests outstanding when connection from localhost/127.0.0.1:8080 is closed
21/02/15 10:17:53 WARN Worker: Failed to connect to master localhost:8080
org.apache.spark.SparkException: Exception thrown in awaitResult: 
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:302)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
	at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:101)
	at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:109)
	at org.apache.spark.deploy.worker.Worker$$anon$1.run(Worker.scala:277)
	at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
	at java.util.concurrent.FutureTask.run(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
Caused by: java.lang.IllegalArgumentException: Too large frame: 5211883372140375593
	at org.sparkproject.guava.base.Preconditions.checkArgument(Preconditions.java:119)
	at org.apache.spark.network.util.TransportFrameDecoder.decodeNext(TransportFrameDecoder.java:148)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:98)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	... 1 more
21/02/15 10:19:08 INFO Worker: Retrying connection to master (attempt # 13)
21/02/15 10:19:08 INFO Worker: Connecting to master localhost:8080...
21/02/15 10:19:08 INFO TransportClientFactory: Found inactive connection to localhost/127.0.0.1:8080, creating a new one.
21/02/15 10:19:08 INFO TransportClientFactory: Successfully created connection to localhost/127.0.0.1:8080 after 1 ms (0 ms spent in bootstraps)
21/02/15 10:19:08 WARN TransportChannelHandler: Exception in connection from localhost/127.0.0.1:8080
java.lang.IllegalArgumentException: Too large frame: 5211883372140375593
	at org.sparkproject.guava.base.Preconditions.checkArgument(Preconditions.java:119)
	at org.apache.spark.network.util.TransportFrameDecoder.decodeNext(TransportFrameDecoder.java:148)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:98)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.lang.Thread.run(Unknown Source)
21/02/15 10:19:08 ERROR TransportResponseHandler: Still have 1 requests outstanding when connection from localhost/127.0.0.1:8080 is closed
21/02/15 10:19:08 WARN Worker: Failed to connect to master localhost:8080
org.apache.spark.SparkException: Exception thrown in awaitResult: 
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:302)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
	at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:101)
	at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:109)
	at org.apache.spark.deploy.worker.Worker$$anon$1.run(Worker.scala:277)
	at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
	at java.util.concurrent.FutureTask.run(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
Caused by: java.lang.IllegalArgumentException: Too large frame: 5211883372140375593
	at org.sparkproject.guava.base.Preconditions.checkArgument(Preconditions.java:119)
	at org.apache.spark.network.util.TransportFrameDecoder.decodeNext(TransportFrameDecoder.java:148)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:98)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	... 1 more
21/02/15 10:20:23 INFO Worker: Retrying connection to master (attempt # 14)
21/02/15 10:20:23 INFO Worker: Connecting to master localhost:8080...
21/02/15 10:20:23 INFO TransportClientFactory: Found inactive connection to localhost/127.0.0.1:8080, creating a new one.
21/02/15 10:20:23 INFO TransportClientFactory: Successfully created connection to localhost/127.0.0.1:8080 after 3 ms (0 ms spent in bootstraps)
21/02/15 10:20:23 WARN TransportChannelHandler: Exception in connection from localhost/127.0.0.1:8080
java.lang.IllegalArgumentException: Too large frame: 5211883372140375593
	at org.sparkproject.guava.base.Preconditions.checkArgument(Preconditions.java:119)
	at org.apache.spark.network.util.TransportFrameDecoder.decodeNext(TransportFrameDecoder.java:148)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:98)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.lang.Thread.run(Unknown Source)
21/02/15 10:20:23 ERROR TransportResponseHandler: Still have 1 requests outstanding when connection from localhost/127.0.0.1:8080 is closed
21/02/15 10:20:23 WARN Worker: Failed to connect to master localhost:8080
org.apache.spark.SparkException: Exception thrown in awaitResult: 
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:302)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
	at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:101)
	at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:109)
	at org.apache.spark.deploy.worker.Worker$$anon$1.run(Worker.scala:277)
	at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
	at java.util.concurrent.FutureTask.run(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
Caused by: java.lang.IllegalArgumentException: Too large frame: 5211883372140375593
	at org.sparkproject.guava.base.Preconditions.checkArgument(Preconditions.java:119)
	at org.apache.spark.network.util.TransportFrameDecoder.decodeNext(TransportFrameDecoder.java:148)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:98)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	... 1 more
21/02/15 10:21:38 INFO Worker: Retrying connection to master (attempt # 15)
21/02/15 10:21:38 INFO Worker: Connecting to master localhost:8080...
21/02/15 10:21:38 INFO TransportClientFactory: Found inactive connection to localhost/127.0.0.1:8080, creating a new one.
21/02/15 10:21:38 INFO TransportClientFactory: Successfully created connection to localhost/127.0.0.1:8080 after 3 ms (0 ms spent in bootstraps)
21/02/15 10:21:38 WARN TransportChannelHandler: Exception in connection from localhost/127.0.0.1:8080
java.lang.IllegalArgumentException: Too large frame: 5211883372140375593
	at org.sparkproject.guava.base.Preconditions.checkArgument(Preconditions.java:119)
	at org.apache.spark.network.util.TransportFrameDecoder.decodeNext(TransportFrameDecoder.java:148)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:98)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.lang.Thread.run(Unknown Source)
21/02/15 10:21:38 ERROR TransportResponseHandler: Still have 1 requests outstanding when connection from localhost/127.0.0.1:8080 is closed
21/02/15 10:21:38 WARN Worker: Failed to connect to master localhost:8080
org.apache.spark.SparkException: Exception thrown in awaitResult: 
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:302)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
	at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:101)
	at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:109)
	at org.apache.spark.deploy.worker.Worker$$anon$1.run(Worker.scala:277)
	at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
	at java.util.concurrent.FutureTask.run(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
Caused by: java.lang.IllegalArgumentException: Too large frame: 5211883372140375593
	at org.sparkproject.guava.base.Preconditions.checkArgument(Preconditions.java:119)
	at org.apache.spark.network.util.TransportFrameDecoder.decodeNext(TransportFrameDecoder.java:148)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:98)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	... 1 more
21/02/15 10:22:53 INFO Worker: Retrying connection to master (attempt # 16)
21/02/15 10:22:53 INFO Worker: Connecting to master localhost:8080...
21/02/15 10:22:53 INFO TransportClientFactory: Found inactive connection to localhost/127.0.0.1:8080, creating a new one.
21/02/15 10:22:53 INFO TransportClientFactory: Successfully created connection to localhost/127.0.0.1:8080 after 5 ms (0 ms spent in bootstraps)
21/02/15 10:22:53 WARN TransportChannelHandler: Exception in connection from localhost/127.0.0.1:8080
java.lang.IllegalArgumentException: Too large frame: 5211883372140375593
	at org.sparkproject.guava.base.Preconditions.checkArgument(Preconditions.java:119)
	at org.apache.spark.network.util.TransportFrameDecoder.decodeNext(TransportFrameDecoder.java:148)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:98)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.lang.Thread.run(Unknown Source)
21/02/15 10:22:53 ERROR TransportResponseHandler: Still have 1 requests outstanding when connection from localhost/127.0.0.1:8080 is closed
21/02/15 10:22:53 WARN Worker: Failed to connect to master localhost:8080
org.apache.spark.SparkException: Exception thrown in awaitResult: 
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:302)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
	at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:101)
	at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:109)
	at org.apache.spark.deploy.worker.Worker$$anon$1.run(Worker.scala:277)
	at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
	at java.util.concurrent.FutureTask.run(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
Caused by: java.lang.IllegalArgumentException: Too large frame: 5211883372140375593
	at org.sparkproject.guava.base.Preconditions.checkArgument(Preconditions.java:119)
	at org.apache.spark.network.util.TransportFrameDecoder.decodeNext(TransportFrameDecoder.java:148)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:98)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	... 1 more
21/02/15 10:24:08 ERROR Worker: All masters are unresponsive! Giving up.
21/02/15 11:17:49 WARN Shell: Did not find winutils.exe: {}
java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:548)
	at org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:569)
	at org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:78)
	at org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1814)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1791)
	at org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)
	at org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)
	at org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:302)
	at org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)
	at org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)
	at org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)
	at org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)
	at org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)
	at org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)
	at org.apache.spark.util.Utils$.createTempDir(Utils.scala:325)
	at org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:343)
	at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:871)
	at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
	at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1007)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1016)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:468)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:439)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:516)
	... 21 more
21/02/15 11:17:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
21/02/15 11:17:49 INFO SecurityManager: Changing view acls to: aaron
21/02/15 11:17:49 INFO SecurityManager: Changing modify acls to: aaron
21/02/15 11:17:49 INFO SecurityManager: Changing view acls groups to: 
21/02/15 11:17:49 INFO SecurityManager: Changing modify acls groups to: 
21/02/15 11:17:49 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(aaron); groups with view permissions: Set(); users  with modify permissions: Set(aaron); groups with modify permissions: Set()
21/02/15 11:17:50 INFO HiveConf: Found configuration file file:/C:/Users/aaron/AppData/Local/spark/spark-3.0.1-bin-hadoop3.2/conf/hive-site.xml
21/02/15 11:17:50 INFO SparkContext: Running Spark version 3.0.1
21/02/15 11:17:50 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
21/02/15 11:17:50 INFO ResourceUtils: ==============================================================
21/02/15 11:17:50 INFO ResourceUtils: Resources for spark.driver:

21/02/15 11:17:50 INFO ResourceUtils: ==============================================================
21/02/15 11:17:50 INFO SparkContext: Submitted application: sparklyr
21/02/15 11:17:50 INFO SecurityManager: Changing view acls to: aaron
21/02/15 11:17:50 INFO SecurityManager: Changing modify acls to: aaron
21/02/15 11:17:50 INFO SecurityManager: Changing view acls groups to: 
21/02/15 11:17:50 INFO SecurityManager: Changing modify acls groups to: 
21/02/15 11:17:50 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(aaron); groups with view permissions: Set(); users  with modify permissions: Set(aaron); groups with modify permissions: Set()
21/02/15 11:17:50 INFO Utils: Successfully started service 'sparkDriver' on port 51935.
21/02/15 11:17:50 INFO SparkEnv: Registering MapOutputTracker
21/02/15 11:17:50 INFO SparkEnv: Registering BlockManagerMaster
21/02/15 11:17:50 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
21/02/15 11:17:50 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
21/02/15 11:17:50 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
21/02/15 11:17:50 INFO DiskBlockManager: Created local directory at C:\Users\aaron\AppData\Local\spark\spark-3.0.1-bin-hadoop3.2\tmp\local\blockmgr-2177f91c-e0ae-441a-89b4-a09fd24a0d2c
21/02/15 11:17:51 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
21/02/15 11:17:51 INFO SparkEnv: Registering OutputCommitCoordinator
21/02/15 11:17:51 WARN Utils: The configured local directories are not expected to be URIs; however, got suspicious values [C:/Users/aaron/AppData/Local/spark/spark-3.0.1-bin-hadoop3.2/tmp/local]. Please check your configured local directories.
21/02/15 11:17:51 INFO Utils: Successfully started service 'SparkUI' on port 4040.
21/02/15 11:17:51 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://host.docker.internal:4040
21/02/15 11:17:51 INFO SparkContext: Added JAR file:/C:/Users/aaron/Documents/R/win-library/4.0/sparklyr/java/sparklyr-3.0-2.12.jar at spark://host.docker.internal:51935/jars/sparklyr-3.0-2.12.jar with timestamp 1613416671315
21/02/15 11:17:51 WARN SparkContext: Please ensure that the number of slots available on your executors is limited by the number of cores to task cpus and not another custom resource. If cores is not the limiting resource then dynamic allocation will not work properly!
21/02/15 11:17:51 ERROR SparkContext: Error initializing SparkContext.
org.apache.spark.SparkException: Could not parse Master URL: '<master>'
	at org.apache.spark.SparkContext$.org$apache$spark$SparkContext$$createTaskScheduler(SparkContext.scala:2944)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:533)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2574)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:934)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:928)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
	at java.lang.reflect.Method.invoke(Unknown Source)
	at sparklyr.Invoke.invoke(invoke.scala:147)
	at sparklyr.StreamHandler.handleMethodCall(stream.scala:136)
	at sparklyr.StreamHandler.read(stream.scala:61)
	at sparklyr.BackendHandler.$anonfun$channelRead0$1(handler.scala:58)
	at scala.util.control.Breaks.breakable(Breaks.scala:42)
	at sparklyr.BackendHandler.channelRead0(handler.scala:39)
	at sparklyr.BackendHandler.channelRead0(handler.scala:14)
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:321)
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:295)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.lang.Thread.run(Unknown Source)
21/02/15 11:17:51 INFO SparkUI: Stopped Spark web UI at http://host.docker.internal:4040
21/02/15 11:17:51 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
21/02/15 11:17:51 INFO MemoryStore: MemoryStore cleared
21/02/15 11:17:51 INFO BlockManager: BlockManager stopped
21/02/15 11:17:51 INFO BlockManagerMaster: BlockManagerMaster stopped
21/02/15 11:17:51 WARN MetricsSystem: Stopping a MetricsSystem that is not running
21/02/15 11:17:51 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
21/02/15 11:17:51 INFO SparkContext: Successfully stopped SparkContext
21/02/15 11:26:33 WARN Shell: Did not find winutils.exe: {}
java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:548)
	at org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:569)
	at org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:78)
	at org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1814)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1791)
	at org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)
	at org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)
	at org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:302)
	at org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)
	at org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)
	at org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)
	at org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)
	at org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)
	at org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)
	at org.apache.spark.util.Utils$.createTempDir(Utils.scala:325)
	at org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:343)
	at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:871)
	at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
	at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1007)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1016)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:468)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:439)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:516)
	... 21 more
21/02/15 11:26:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
21/02/15 11:26:33 INFO SecurityManager: Changing view acls to: aaron
21/02/15 11:26:33 INFO SecurityManager: Changing modify acls to: aaron
21/02/15 11:26:33 INFO SecurityManager: Changing view acls groups to: 
21/02/15 11:26:33 INFO SecurityManager: Changing modify acls groups to: 
21/02/15 11:26:33 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(aaron); groups with view permissions: Set(); users  with modify permissions: Set(aaron); groups with modify permissions: Set()
21/02/15 11:26:35 INFO HiveConf: Found configuration file file:/C:/Users/aaron/AppData/Local/spark/spark-3.0.1-bin-hadoop3.2/conf/hive-site.xml
21/02/15 11:26:35 INFO SparkContext: Running Spark version 3.0.1
21/02/15 11:26:35 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
21/02/15 11:26:35 INFO ResourceUtils: ==============================================================
21/02/15 11:26:35 INFO ResourceUtils: Resources for spark.driver:

21/02/15 11:26:35 INFO ResourceUtils: ==============================================================
21/02/15 11:26:35 INFO SparkContext: Submitted application: sparklyr
21/02/15 11:26:35 INFO SecurityManager: Changing view acls to: aaron
21/02/15 11:26:35 INFO SecurityManager: Changing modify acls to: aaron
21/02/15 11:26:35 INFO SecurityManager: Changing view acls groups to: 
21/02/15 11:26:35 INFO SecurityManager: Changing modify acls groups to: 
21/02/15 11:26:35 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(aaron); groups with view permissions: Set(); users  with modify permissions: Set(aaron); groups with modify permissions: Set()
21/02/15 11:26:35 INFO Utils: Successfully started service 'sparkDriver' on port 52605.
21/02/15 11:26:35 INFO SparkEnv: Registering MapOutputTracker
21/02/15 11:26:35 INFO SparkEnv: Registering BlockManagerMaster
21/02/15 11:26:35 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
21/02/15 11:26:35 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
21/02/15 11:26:35 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
21/02/15 11:26:35 INFO DiskBlockManager: Created local directory at C:\Users\aaron\AppData\Local\spark\spark-3.0.1-bin-hadoop3.2\tmp\local\blockmgr-7fab079f-fee8-4b0e-9512-63fafb0a3cb7
21/02/15 11:26:35 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
21/02/15 11:26:35 INFO SparkEnv: Registering OutputCommitCoordinator
21/02/15 11:26:35 WARN Utils: The configured local directories are not expected to be URIs; however, got suspicious values [C:/Users/aaron/AppData/Local/spark/spark-3.0.1-bin-hadoop3.2/tmp/local]. Please check your configured local directories.
21/02/15 11:26:36 INFO Utils: Successfully started service 'SparkUI' on port 4040.
21/02/15 11:26:36 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://host.docker.internal:4040
21/02/15 11:26:36 INFO SparkContext: Added JAR file:/C:/Users/aaron/Documents/R/win-library/4.0/sparklyr/java/sparklyr-3.0-2.12.jar at spark://host.docker.internal:52605/jars/sparklyr-3.0-2.12.jar with timestamp 1613417196120
21/02/15 11:26:36 WARN SparkContext: Please ensure that the number of slots available on your executors is limited by the number of cores to task cpus and not another custom resource. If cores is not the limiting resource then dynamic allocation will not work properly!
21/02/15 11:26:36 ERROR SparkContext: Error initializing SparkContext.
org.apache.spark.SparkException: Could not parse Master URL: '<master>'
	at org.apache.spark.SparkContext$.org$apache$spark$SparkContext$$createTaskScheduler(SparkContext.scala:2944)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:533)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2574)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:934)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:928)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
	at java.lang.reflect.Method.invoke(Unknown Source)
	at sparklyr.Invoke.invoke(invoke.scala:147)
	at sparklyr.StreamHandler.handleMethodCall(stream.scala:136)
	at sparklyr.StreamHandler.read(stream.scala:61)
	at sparklyr.BackendHandler.$anonfun$channelRead0$1(handler.scala:58)
	at scala.util.control.Breaks.breakable(Breaks.scala:42)
	at sparklyr.BackendHandler.channelRead0(handler.scala:39)
	at sparklyr.BackendHandler.channelRead0(handler.scala:14)
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:321)
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:295)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.lang.Thread.run(Unknown Source)
21/02/15 11:26:36 INFO SparkUI: Stopped Spark web UI at http://host.docker.internal:4040
21/02/15 11:26:36 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
21/02/15 11:26:36 INFO MemoryStore: MemoryStore cleared
21/02/15 11:26:36 INFO BlockManager: BlockManager stopped
21/02/15 11:26:36 INFO BlockManagerMaster: BlockManagerMaster stopped
21/02/15 11:26:36 WARN MetricsSystem: Stopping a MetricsSystem that is not running
21/02/15 11:26:36 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
21/02/15 11:26:36 INFO SparkContext: Successfully stopped SparkContext
21/02/15 11:26:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
21/02/15 11:26:50 INFO SecurityManager: Changing view acls to: aaron
21/02/15 11:26:50 INFO SecurityManager: Changing modify acls to: aaron
21/02/15 11:26:50 INFO SecurityManager: Changing view acls groups to: 
21/02/15 11:26:50 INFO SecurityManager: Changing modify acls groups to: 
21/02/15 11:26:50 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(aaron); groups with view permissions: Set(); users  with modify permissions: Set(aaron); groups with modify permissions: Set()
21/02/15 11:26:51 INFO HiveConf: Found configuration file file:/C:/Users/aaron/AppData/Local/spark/spark-3.0.1-bin-hadoop3.2/conf/hive-site.xml
21/02/15 11:26:52 INFO SparkContext: Running Spark version 3.0.1
21/02/15 11:26:52 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
21/02/15 11:26:52 INFO ResourceUtils: ==============================================================
21/02/15 11:26:52 INFO ResourceUtils: Resources for spark.driver:

21/02/15 11:26:52 INFO ResourceUtils: ==============================================================
21/02/15 11:26:52 INFO SparkContext: Submitted application: sparklyr
21/02/15 11:26:52 INFO SecurityManager: Changing view acls to: aaron
21/02/15 11:26:52 INFO SecurityManager: Changing modify acls to: aaron
21/02/15 11:26:52 INFO SecurityManager: Changing view acls groups to: 
21/02/15 11:26:52 INFO SecurityManager: Changing modify acls groups to: 
21/02/15 11:26:52 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(aaron); groups with view permissions: Set(); users  with modify permissions: Set(aaron); groups with modify permissions: Set()
21/02/15 11:26:52 INFO Utils: Successfully started service 'sparkDriver' on port 52692.
21/02/15 11:26:52 INFO SparkEnv: Registering MapOutputTracker
21/02/15 11:26:52 INFO SparkEnv: Registering BlockManagerMaster
21/02/15 11:26:52 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
21/02/15 11:26:52 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
21/02/15 11:26:52 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
21/02/15 11:26:52 INFO DiskBlockManager: Created local directory at C:\Users\aaron\AppData\Local\spark\spark-3.0.1-bin-hadoop3.2\tmp\local\blockmgr-0c230d21-3721-4222-952d-1143a72ef509
21/02/15 11:26:52 INFO MemoryStore: MemoryStore started with capacity 912.3 MiB
21/02/15 11:26:52 INFO SparkEnv: Registering OutputCommitCoordinator
21/02/15 11:26:52 WARN Utils: The configured local directories are not expected to be URIs; however, got suspicious values [C:/Users/aaron/AppData/Local/spark/spark-3.0.1-bin-hadoop3.2/tmp/local]. Please check your configured local directories.
21/02/15 11:26:52 INFO Utils: Successfully started service 'SparkUI' on port 4040.
21/02/15 11:26:52 INFO SparkUI: Bound SparkUI to 127.0.0.1, and started at http://view-localhost:4040
21/02/15 11:26:52 INFO SparkContext: Added JAR file:/C:/Users/aaron/Documents/R/win-library/4.0/sparklyr/java/sparklyr-3.0-2.12.jar at spark://view-localhost:52692/jars/sparklyr-3.0-2.12.jar with timestamp 1613417212726
21/02/15 11:26:52 INFO Executor: Starting executor ID driver on host view-localhost
21/02/15 11:26:52 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 52739.
21/02/15 11:26:52 INFO NettyBlockTransferService: Server created on view-localhost:52739
21/02/15 11:26:52 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
21/02/15 11:26:52 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, view-localhost, 52739, None)
21/02/15 11:26:52 INFO BlockManagerMasterEndpoint: Registering block manager view-localhost:52739 with 912.3 MiB RAM, BlockManagerId(driver, view-localhost, 52739, None)
21/02/15 11:26:52 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, view-localhost, 52739, None)
21/02/15 11:26:52 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, view-localhost, 52739, None)
21/02/15 11:26:53 INFO SharedState: loading hive config file: file:/C:/Users/aaron/AppData/Local/spark/spark-3.0.1-bin-hadoop3.2/conf/hive-site.xml
21/02/15 11:26:53 INFO SharedState: Setting hive.metastore.warehouse.dir ('C:/Users/aaron/AppData/Local/spark/spark-3.0.1-bin-hadoop3.2/tmp/hive') to the value of spark.sql.warehouse.dir ('C:/Users/aaron/AppData/Local/spark/spark-3.0.1-bin-hadoop3.2/tmp/hive').
21/02/15 11:26:53 INFO SharedState: Warehouse path is 'C:/Users/aaron/AppData/Local/spark/spark-3.0.1-bin-hadoop3.2/tmp/hive'.
21/02/15 11:26:53 WARN SharedState: Not allowing to set spark.sql.warehouse.dir or hive.metastore.warehouse.dir in SparkSession's options, it should be set statically for cross-session usages
21/02/15 11:26:55 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.7 using Spark classes.
21/02/15 11:26:56 INFO HiveConf: Found configuration file file:/C:/Users/aaron/AppData/Local/spark/spark-3.0.1-bin-hadoop3.2/conf/hive-site.xml
21/02/15 11:26:56 INFO SessionState: Created HDFS directory: C:/Users/aaron/AppData/Local/spark/spark-3.0.1-bin-hadoop3.2/tmp/hive/aaron
21/02/15 11:26:56 INFO SessionState: Created HDFS directory: C:/Users/aaron/AppData/Local/spark/spark-3.0.1-bin-hadoop3.2/tmp/hive/aaron/6e0f7d64-4e05-4d4d-9bd0-09971e772a50
21/02/15 11:26:56 INFO SessionState: Created local directory: C:/Users/aaron/AppData/Local/spark/spark-3.0.1-bin-hadoop3.2/tmp/hive/6e0f7d64-4e05-4d4d-9bd0-09971e772a50
21/02/15 11:26:56 INFO SessionState: Created HDFS directory: C:/Users/aaron/AppData/Local/spark/spark-3.0.1-bin-hadoop3.2/tmp/hive/aaron/6e0f7d64-4e05-4d4d-9bd0-09971e772a50/_tmp_space.db
21/02/15 11:26:56 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.7) is C:/Users/aaron/AppData/Local/spark/spark-3.0.1-bin-hadoop3.2/tmp/hive
21/02/15 11:26:57 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
21/02/15 11:26:57 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
21/02/15 11:26:57 INFO HiveMetaStore: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
21/02/15 11:26:57 INFO ObjectStore: ObjectStore, initialize called
21/02/15 11:26:57 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
21/02/15 11:26:57 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
21/02/15 11:26:58 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
21/02/15 11:26:59 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
21/02/15 11:26:59 INFO ObjectStore: Initialized ObjectStore
21/02/15 11:26:59 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0
21/02/15 11:26:59 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore UNKNOWN@192.168.0.31
21/02/15 11:26:59 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
21/02/15 11:26:59 INFO HiveMetaStore: Added admin role in metastore
21/02/15 11:26:59 INFO HiveMetaStore: Added public role in metastore
21/02/15 11:26:59 INFO HiveMetaStore: No user is added in admin role, since config is empty
21/02/15 11:26:59 INFO HiveMetaStore: 0: get_all_functions
21/02/15 11:26:59 INFO audit: ugi=aaron	ip=unknown-ip-addr	cmd=get_all_functions	
21/02/15 11:27:00 INFO HiveMetaStore: 0: get_database: default
21/02/15 11:27:00 INFO audit: ugi=aaron	ip=unknown-ip-addr	cmd=get_database: default	
21/02/15 11:27:00 INFO HiveMetaStore: 0: get_database: global_temp
21/02/15 11:27:00 INFO audit: ugi=aaron	ip=unknown-ip-addr	cmd=get_database: global_temp	
21/02/15 11:27:00 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
21/02/15 11:27:00 INFO HiveMetaStore: 0: get_database: default
21/02/15 11:27:00 INFO audit: ugi=aaron	ip=unknown-ip-addr	cmd=get_database: default	
21/02/15 11:27:00 INFO HiveMetaStore: 0: get_database: default
21/02/15 11:27:00 INFO audit: ugi=aaron	ip=unknown-ip-addr	cmd=get_database: default	
21/02/15 11:27:00 INFO HiveMetaStore: 0: get_tables: db=default pat=*
21/02/15 11:27:00 INFO audit: ugi=aaron	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
21/02/15 11:27:00 INFO CodeGenerator: Code generated in 211.9779 ms
21/02/15 11:27:00 INFO CodeGenerator: Code generated in 8.9064 ms
21/02/15 11:27:01 INFO SparkContext: Starting job: count at utils.scala:116
21/02/15 11:27:01 INFO DAGScheduler: Registering RDD 2 (count at utils.scala:116) as input to shuffle 0
21/02/15 11:27:01 INFO DAGScheduler: Got job 0 (count at utils.scala:116) with 1 output partitions
21/02/15 11:27:01 INFO DAGScheduler: Final stage: ResultStage 1 (count at utils.scala:116)
21/02/15 11:27:01 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
21/02/15 11:27:01 INFO DAGScheduler: Missing parents: List()
21/02/15 11:27:01 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[5] at count at utils.scala:116), which has no missing parents
21/02/15 11:27:01 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 10.1 KiB, free 912.3 MiB)
21/02/15 11:27:01 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 5.0 KiB, free 912.3 MiB)
21/02/15 11:27:01 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on view-localhost:52739 (size: 5.0 KiB, free: 912.3 MiB)
21/02/15 11:27:01 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1223
21/02/15 11:27:01 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at count at utils.scala:116) (first 15 tasks are for partitions Vector(0))
21/02/15 11:27:01 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
21/02/15 11:27:01 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 0, view-localhost, executor driver, partition 0, PROCESS_LOCAL, 7325 bytes)
21/02/15 11:27:01 INFO Executor: Running task 0.0 in stage 1.0 (TID 0)
21/02/15 11:27:01 INFO Executor: Fetching spark://view-localhost:52692/jars/sparklyr-3.0-2.12.jar with timestamp 1613417212726
21/02/15 11:27:01 INFO TransportClientFactory: Successfully created connection to view-localhost/127.0.0.1:52692 after 19 ms (0 ms spent in bootstraps)
21/02/15 11:27:01 INFO Utils: Fetching spark://view-localhost:52692/jars/sparklyr-3.0-2.12.jar to C:\Users\aaron\AppData\Local\spark\spark-3.0.1-bin-hadoop3.2\tmp\local\spark-3f6a6276-2282-4656-a7ae-2190abfc3f93\userFiles-d2e2a90f-01c1-48d1-ab48-ec377a0e503c\fetchFileTemp7739827700448362396.tmp
21/02/15 11:27:01 INFO Executor: Adding file:/C:/Users/aaron/AppData/Local/spark/spark-3.0.1-bin-hadoop3.2/tmp/local/spark-3f6a6276-2282-4656-a7ae-2190abfc3f93/userFiles-d2e2a90f-01c1-48d1-ab48-ec377a0e503c/sparklyr-3.0-2.12.jar to class loader
21/02/15 11:27:01 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/02/15 11:27:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
21/02/15 11:27:01 INFO Executor: Finished task 0.0 in stage 1.0 (TID 0). 2641 bytes result sent to driver
21/02/15 11:27:01 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 0) in 362 ms on view-localhost (executor driver) (1/1)
21/02/15 11:27:01 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
21/02/15 11:27:01 INFO DAGScheduler: ResultStage 1 (count at utils.scala:116) finished in 0.504 s
21/02/15 11:27:01 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
21/02/15 11:27:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
21/02/15 11:27:01 INFO DAGScheduler: Job 0 finished: count at utils.scala:116, took 0.545466 s
21/02/15 11:27:01 INFO HiveMetaStore: 0: get_database: default
21/02/15 11:27:01 INFO audit: ugi=aaron	ip=unknown-ip-addr	cmd=get_database: default	
21/02/15 11:27:01 INFO HiveMetaStore: 0: get_database: default
21/02/15 11:27:01 INFO audit: ugi=aaron	ip=unknown-ip-addr	cmd=get_database: default	
21/02/15 11:27:01 INFO HiveMetaStore: 0: get_tables: db=default pat=*
21/02/15 11:27:01 INFO audit: ugi=aaron	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
21/02/15 11:27:01 INFO SparkContext: Starting job: count at utils.scala:116
21/02/15 11:27:01 INFO DAGScheduler: Registering RDD 8 (count at utils.scala:116) as input to shuffle 1
21/02/15 11:27:01 INFO DAGScheduler: Got job 1 (count at utils.scala:116) with 1 output partitions
21/02/15 11:27:01 INFO DAGScheduler: Final stage: ResultStage 3 (count at utils.scala:116)
21/02/15 11:27:01 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
21/02/15 11:27:01 INFO DAGScheduler: Missing parents: List()
21/02/15 11:27:01 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[11] at count at utils.scala:116), which has no missing parents
21/02/15 11:27:01 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 10.1 KiB, free 912.3 MiB)
21/02/15 11:27:01 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.0 KiB, free 912.3 MiB)
21/02/15 11:27:01 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on view-localhost:52739 (size: 5.0 KiB, free: 912.3 MiB)
21/02/15 11:27:01 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1223
21/02/15 11:27:01 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[11] at count at utils.scala:116) (first 15 tasks are for partitions Vector(0))
21/02/15 11:27:01 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks
21/02/15 11:27:01 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 1, view-localhost, executor driver, partition 0, PROCESS_LOCAL, 7325 bytes)
21/02/15 11:27:01 INFO Executor: Running task 0.0 in stage 3.0 (TID 1)
21/02/15 11:27:01 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/02/15 11:27:01 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
21/02/15 11:27:01 INFO Executor: Finished task 0.0 in stage 3.0 (TID 1). 2598 bytes result sent to driver
21/02/15 11:27:01 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 1) in 7 ms on view-localhost (executor driver) (1/1)
21/02/15 11:27:01 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
21/02/15 11:27:01 INFO DAGScheduler: ResultStage 3 (count at utils.scala:116) finished in 0.012 s
21/02/15 11:27:01 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
21/02/15 11:27:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
21/02/15 11:27:01 INFO DAGScheduler: Job 1 finished: count at utils.scala:116, took 0.015781 s
21/02/15 11:27:05 WARN ProcfsMetricsGetter: Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped
21/02/15 11:39:59 INFO HiveMetaStore: 0: get_database: default
21/02/15 11:39:59 INFO audit: ugi=aaron	ip=unknown-ip-addr	cmd=get_database: default	
21/02/15 11:39:59 INFO HiveMetaStore: 0: get_database: default
21/02/15 11:39:59 INFO audit: ugi=aaron	ip=unknown-ip-addr	cmd=get_database: default	
21/02/15 11:39:59 INFO HiveMetaStore: 0: get_tables: db=default pat=*
21/02/15 11:39:59 INFO audit: ugi=aaron	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
21/02/15 11:40:00 INFO BlockManagerInfo: Removed broadcast_0_piece0 on view-localhost:52739 in memory (size: 5.0 KiB, free: 912.3 MiB)
21/02/15 11:40:00 INFO SparkContext: Starting job: count at utils.scala:116
21/02/15 11:40:00 INFO DAGScheduler: Registering RDD 14 (count at utils.scala:116) as input to shuffle 2
21/02/15 11:40:00 INFO DAGScheduler: Got job 2 (count at utils.scala:116) with 1 output partitions
21/02/15 11:40:00 INFO DAGScheduler: Final stage: ResultStage 5 (count at utils.scala:116)
21/02/15 11:40:00 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 4)
21/02/15 11:40:00 INFO DAGScheduler: Missing parents: List()
21/02/15 11:40:00 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[17] at count at utils.scala:116), which has no missing parents
21/02/15 11:40:00 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 10.1 KiB, free 912.3 MiB)
21/02/15 11:40:00 INFO BlockManagerInfo: Removed broadcast_1_piece0 on view-localhost:52739 in memory (size: 5.0 KiB, free: 912.3 MiB)
21/02/15 11:40:00 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.0 KiB, free 912.3 MiB)
21/02/15 11:40:00 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on view-localhost:52739 (size: 5.0 KiB, free: 912.3 MiB)
21/02/15 11:40:00 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1223
21/02/15 11:40:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[17] at count at utils.scala:116) (first 15 tasks are for partitions Vector(0))
21/02/15 11:40:00 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks
21/02/15 11:40:00 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 2, view-localhost, executor driver, partition 0, PROCESS_LOCAL, 7325 bytes)
21/02/15 11:40:00 INFO Executor: Running task 0.0 in stage 5.0 (TID 2)
21/02/15 11:40:00 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/02/15 11:40:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
21/02/15 11:40:00 INFO Executor: Finished task 0.0 in stage 5.0 (TID 2). 2555 bytes result sent to driver
21/02/15 11:40:00 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 2) in 10 ms on view-localhost (executor driver) (1/1)
21/02/15 11:40:00 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
21/02/15 11:40:00 INFO DAGScheduler: ResultStage 5 (count at utils.scala:116) finished in 0.017 s
21/02/15 11:40:00 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
21/02/15 11:40:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
21/02/15 11:40:00 INFO DAGScheduler: Job 2 finished: count at utils.scala:116, took 0.021278 s
21/02/15 11:40:00 INFO HiveMetaStore: 0: get_database: default
21/02/15 11:40:00 INFO audit: ugi=aaron	ip=unknown-ip-addr	cmd=get_database: default	
21/02/15 11:40:00 INFO HiveMetaStore: 0: get_database: default
21/02/15 11:40:00 INFO audit: ugi=aaron	ip=unknown-ip-addr	cmd=get_database: default	
21/02/15 11:40:00 INFO HiveMetaStore: 0: get_tables: db=default pat=*
21/02/15 11:40:00 INFO audit: ugi=aaron	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
21/02/15 11:40:00 INFO SparkContext: Starting job: count at utils.scala:116
21/02/15 11:40:00 INFO DAGScheduler: Registering RDD 20 (count at utils.scala:116) as input to shuffle 3
21/02/15 11:40:00 INFO DAGScheduler: Got job 3 (count at utils.scala:116) with 1 output partitions
21/02/15 11:40:00 INFO DAGScheduler: Final stage: ResultStage 7 (count at utils.scala:116)
21/02/15 11:40:00 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 6)
21/02/15 11:40:00 INFO DAGScheduler: Missing parents: List()
21/02/15 11:40:00 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[23] at count at utils.scala:116), which has no missing parents
21/02/15 11:40:00 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 10.1 KiB, free 912.3 MiB)
21/02/15 11:40:00 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 5.0 KiB, free 912.3 MiB)
21/02/15 11:40:00 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on view-localhost:52739 (size: 5.0 KiB, free: 912.3 MiB)
21/02/15 11:40:00 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1223
21/02/15 11:40:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[23] at count at utils.scala:116) (first 15 tasks are for partitions Vector(0))
21/02/15 11:40:00 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks
21/02/15 11:40:00 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 3, view-localhost, executor driver, partition 0, PROCESS_LOCAL, 7325 bytes)
21/02/15 11:40:00 INFO Executor: Running task 0.0 in stage 7.0 (TID 3)
21/02/15 11:40:00 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/02/15 11:40:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
21/02/15 11:40:00 INFO Executor: Finished task 0.0 in stage 7.0 (TID 3). 2555 bytes result sent to driver
21/02/15 11:40:00 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 3) in 7 ms on view-localhost (executor driver) (1/1)
21/02/15 11:40:00 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool 
21/02/15 11:40:00 INFO DAGScheduler: ResultStage 7 (count at utils.scala:116) finished in 0.014 s
21/02/15 11:40:00 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
21/02/15 11:40:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
21/02/15 11:40:00 INFO DAGScheduler: Job 3 finished: count at utils.scala:116, took 0.016874 s
21/02/15 11:40:03 INFO SparkContext: Invoking stop() from shutdown hook
21/02/15 11:40:03 INFO SparkUI: Stopped Spark web UI at http://view-localhost:4040
21/02/15 11:40:03 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
21/02/15 11:40:03 INFO MemoryStore: MemoryStore cleared
21/02/15 11:40:03 INFO BlockManager: BlockManager stopped
21/02/15 11:40:03 INFO BlockManagerMaster: BlockManagerMaster stopped
21/02/15 11:40:03 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
21/02/15 11:40:03 INFO SparkContext: Successfully stopped SparkContext
21/02/15 11:40:03 INFO ShutdownHookManager: Shutdown hook called
21/02/15 11:40:03 INFO ShutdownHookManager: Deleting directory C:\Users\aaron\AppData\Local\spark\spark-3.0.1-bin-hadoop3.2\tmp\local\spark-3f6a6276-2282-4656-a7ae-2190abfc3f93
21/02/15 11:40:03 INFO ShutdownHookManager: Deleting directory C:\Users\aaron\AppData\Local\Temp\spark-c1f124e4-d905-41d7-95aa-c512e43e76fb
21/02/15 11:43:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
21/02/15 11:43:20 INFO SecurityManager: Changing view acls to: aaron
21/02/15 11:43:20 INFO SecurityManager: Changing modify acls to: aaron
21/02/15 11:43:20 INFO SecurityManager: Changing view acls groups to: 
21/02/15 11:43:20 INFO SecurityManager: Changing modify acls groups to: 
21/02/15 11:43:20 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(aaron); groups with view permissions: Set(); users  with modify permissions: Set(aaron); groups with modify permissions: Set()
21/02/15 11:43:21 INFO HiveConf: Found configuration file file:/C:/Users/aaron/AppData/Local/spark/spark-3.0.1-bin-hadoop3.2/conf/hive-site.xml
21/02/15 11:43:21 INFO SparkContext: Running Spark version 3.0.1
21/02/15 11:43:21 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
21/02/15 11:43:21 INFO ResourceUtils: ==============================================================
21/02/15 11:43:21 INFO ResourceUtils: Resources for spark.driver:

21/02/15 11:43:21 INFO ResourceUtils: ==============================================================
21/02/15 11:43:21 INFO SparkContext: Submitted application: sparklyr
21/02/15 11:43:21 INFO SecurityManager: Changing view acls to: aaron
21/02/15 11:43:21 INFO SecurityManager: Changing modify acls to: aaron
21/02/15 11:43:21 INFO SecurityManager: Changing view acls groups to: 
21/02/15 11:43:21 INFO SecurityManager: Changing modify acls groups to: 
21/02/15 11:43:21 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(aaron); groups with view permissions: Set(); users  with modify permissions: Set(aaron); groups with modify permissions: Set()
21/02/15 11:43:21 INFO Utils: Successfully started service 'sparkDriver' on port 53397.
21/02/15 11:43:21 INFO SparkEnv: Registering MapOutputTracker
21/02/15 11:43:21 INFO SparkEnv: Registering BlockManagerMaster
21/02/15 11:43:21 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
21/02/15 11:43:21 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
21/02/15 11:43:21 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
21/02/15 11:43:21 INFO DiskBlockManager: Created local directory at C:\Users\aaron\AppData\Local\spark\spark-3.0.1-bin-hadoop3.2\tmp\local\blockmgr-cc746f1a-1ef1-47b3-87fb-0eb1c144eb11
21/02/15 11:43:21 INFO MemoryStore: MemoryStore started with capacity 912.3 MiB
21/02/15 11:43:21 INFO SparkEnv: Registering OutputCommitCoordinator
21/02/15 11:43:22 WARN Utils: The configured local directories are not expected to be URIs; however, got suspicious values [C:/Users/aaron/AppData/Local/spark/spark-3.0.1-bin-hadoop3.2/tmp/local]. Please check your configured local directories.
21/02/15 11:43:22 INFO Utils: Successfully started service 'SparkUI' on port 4040.
21/02/15 11:43:22 INFO SparkUI: Bound SparkUI to 127.0.0.1, and started at http://view-localhost:4040
21/02/15 11:43:22 INFO SparkContext: Added JAR file:/C:/Users/aaron/Documents/R/win-library/4.0/sparklyr/java/sparklyr-3.0-2.12.jar at spark://view-localhost:53397/jars/sparklyr-3.0-2.12.jar with timestamp 1613418202274
21/02/15 11:43:22 INFO Executor: Starting executor ID driver on host view-localhost
21/02/15 11:43:22 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 53444.
21/02/15 11:43:22 INFO NettyBlockTransferService: Server created on view-localhost:53444
21/02/15 11:43:22 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
21/02/15 11:43:22 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, view-localhost, 53444, None)
21/02/15 11:43:22 INFO BlockManagerMasterEndpoint: Registering block manager view-localhost:53444 with 912.3 MiB RAM, BlockManagerId(driver, view-localhost, 53444, None)
21/02/15 11:43:22 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, view-localhost, 53444, None)
21/02/15 11:43:22 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, view-localhost, 53444, None)
21/02/15 11:43:22 INFO SharedState: loading hive config file: file:/C:/Users/aaron/AppData/Local/spark/spark-3.0.1-bin-hadoop3.2/conf/hive-site.xml
21/02/15 11:43:22 INFO SharedState: Setting hive.metastore.warehouse.dir ('C:/Users/aaron/AppData/Local/spark/spark-3.0.1-bin-hadoop3.2/tmp/hive') to the value of spark.sql.warehouse.dir ('C:/Users/aaron/AppData/Local/spark/spark-3.0.1-bin-hadoop3.2/tmp/hive').
21/02/15 11:43:22 INFO SharedState: Warehouse path is 'C:/Users/aaron/AppData/Local/spark/spark-3.0.1-bin-hadoop3.2/tmp/hive'.
21/02/15 11:43:22 WARN SharedState: Not allowing to set spark.sql.warehouse.dir or hive.metastore.warehouse.dir in SparkSession's options, it should be set statically for cross-session usages
21/02/15 11:43:25 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.7 using Spark classes.
21/02/15 11:43:25 INFO HiveConf: Found configuration file file:/C:/Users/aaron/AppData/Local/spark/spark-3.0.1-bin-hadoop3.2/conf/hive-site.xml
21/02/15 11:43:25 INFO SessionState: Created HDFS directory: C:/Users/aaron/AppData/Local/spark/spark-3.0.1-bin-hadoop3.2/tmp/hive/aaron/a3141f82-fe8e-4f15-bd05-ba8e9c1942f9
21/02/15 11:43:25 INFO SessionState: Created local directory: C:/Users/aaron/AppData/Local/spark/spark-3.0.1-bin-hadoop3.2/tmp/hive/a3141f82-fe8e-4f15-bd05-ba8e9c1942f9
21/02/15 11:43:25 INFO SessionState: Created HDFS directory: C:/Users/aaron/AppData/Local/spark/spark-3.0.1-bin-hadoop3.2/tmp/hive/aaron/a3141f82-fe8e-4f15-bd05-ba8e9c1942f9/_tmp_space.db
21/02/15 11:43:25 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.7) is C:/Users/aaron/AppData/Local/spark/spark-3.0.1-bin-hadoop3.2/tmp/hive
21/02/15 11:43:26 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
21/02/15 11:43:26 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
21/02/15 11:43:26 INFO HiveMetaStore: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
21/02/15 11:43:26 INFO ObjectStore: ObjectStore, initialize called
21/02/15 11:43:26 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
21/02/15 11:43:26 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
21/02/15 11:43:27 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
21/02/15 11:43:28 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
21/02/15 11:43:28 INFO ObjectStore: Initialized ObjectStore
21/02/15 11:43:28 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0
21/02/15 11:43:28 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore UNKNOWN@192.168.0.31
21/02/15 11:43:28 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
21/02/15 11:43:28 INFO HiveMetaStore: Added admin role in metastore
21/02/15 11:43:28 INFO HiveMetaStore: Added public role in metastore
21/02/15 11:43:28 INFO HiveMetaStore: No user is added in admin role, since config is empty
21/02/15 11:43:28 INFO HiveMetaStore: 0: get_all_functions
21/02/15 11:43:28 INFO audit: ugi=aaron	ip=unknown-ip-addr	cmd=get_all_functions	
21/02/15 11:43:29 INFO HiveMetaStore: 0: get_database: default
21/02/15 11:43:29 INFO audit: ugi=aaron	ip=unknown-ip-addr	cmd=get_database: default	
21/02/15 11:43:29 INFO HiveMetaStore: 0: get_database: global_temp
21/02/15 11:43:29 INFO audit: ugi=aaron	ip=unknown-ip-addr	cmd=get_database: global_temp	
21/02/15 11:43:29 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
21/02/15 11:43:29 INFO HiveMetaStore: 0: get_database: default
21/02/15 11:43:29 INFO audit: ugi=aaron	ip=unknown-ip-addr	cmd=get_database: default	
21/02/15 11:43:29 INFO HiveMetaStore: 0: get_database: default
21/02/15 11:43:29 INFO audit: ugi=aaron	ip=unknown-ip-addr	cmd=get_database: default	
21/02/15 11:43:29 INFO HiveMetaStore: 0: get_tables: db=default pat=*
21/02/15 11:43:29 INFO audit: ugi=aaron	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
21/02/15 11:43:29 INFO CodeGenerator: Code generated in 180.2909 ms
21/02/15 11:43:29 INFO CodeGenerator: Code generated in 9.0483 ms
21/02/15 11:43:30 INFO SparkContext: Starting job: count at utils.scala:116
21/02/15 11:43:30 INFO DAGScheduler: Registering RDD 2 (count at utils.scala:116) as input to shuffle 0
21/02/15 11:43:30 INFO DAGScheduler: Got job 0 (count at utils.scala:116) with 1 output partitions
21/02/15 11:43:30 INFO DAGScheduler: Final stage: ResultStage 1 (count at utils.scala:116)
21/02/15 11:43:30 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
21/02/15 11:43:30 INFO DAGScheduler: Missing parents: List()
21/02/15 11:43:30 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[5] at count at utils.scala:116), which has no missing parents
21/02/15 11:43:30 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 10.1 KiB, free 912.3 MiB)
21/02/15 11:43:30 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 5.0 KiB, free 912.3 MiB)
21/02/15 11:43:30 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on view-localhost:53444 (size: 5.0 KiB, free: 912.3 MiB)
21/02/15 11:43:30 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1223
21/02/15 11:43:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at count at utils.scala:116) (first 15 tasks are for partitions Vector(0))
21/02/15 11:43:30 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
21/02/15 11:43:30 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 0, view-localhost, executor driver, partition 0, PROCESS_LOCAL, 7325 bytes)
21/02/15 11:43:30 INFO Executor: Running task 0.0 in stage 1.0 (TID 0)
21/02/15 11:43:30 INFO Executor: Fetching spark://view-localhost:53397/jars/sparklyr-3.0-2.12.jar with timestamp 1613418202274
21/02/15 11:43:30 INFO TransportClientFactory: Successfully created connection to view-localhost/127.0.0.1:53397 after 17 ms (0 ms spent in bootstraps)
21/02/15 11:43:30 INFO Utils: Fetching spark://view-localhost:53397/jars/sparklyr-3.0-2.12.jar to C:\Users\aaron\AppData\Local\spark\spark-3.0.1-bin-hadoop3.2\tmp\local\spark-1c991ee0-771c-44c3-a9fc-fc592cc13f0f\userFiles-9bc9b9d3-9dde-4720-b8ab-4fb86374448c\fetchFileTemp1532290569023306177.tmp
21/02/15 11:43:30 INFO Executor: Adding file:/C:/Users/aaron/AppData/Local/spark/spark-3.0.1-bin-hadoop3.2/tmp/local/spark-1c991ee0-771c-44c3-a9fc-fc592cc13f0f/userFiles-9bc9b9d3-9dde-4720-b8ab-4fb86374448c/sparklyr-3.0-2.12.jar to class loader
21/02/15 11:43:30 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/02/15 11:43:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
21/02/15 11:43:30 INFO Executor: Finished task 0.0 in stage 1.0 (TID 0). 2641 bytes result sent to driver
21/02/15 11:43:30 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 0) in 364 ms on view-localhost (executor driver) (1/1)
21/02/15 11:43:30 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
21/02/15 11:43:30 INFO DAGScheduler: ResultStage 1 (count at utils.scala:116) finished in 0.511 s
21/02/15 11:43:30 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
21/02/15 11:43:30 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
21/02/15 11:43:30 INFO DAGScheduler: Job 0 finished: count at utils.scala:116, took 0.549082 s
21/02/15 11:43:34 WARN ProcfsMetricsGetter: Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped
21/02/15 11:43:38 INFO BlockManagerInfo: Removed broadcast_0_piece0 on view-localhost:53444 in memory (size: 5.0 KiB, free: 912.3 MiB)
21/02/15 11:44:14 INFO SparkContext: Invoking stop() from shutdown hook
21/02/15 11:44:14 INFO SparkUI: Stopped Spark web UI at http://view-localhost:4040
21/02/15 11:44:14 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
21/02/15 11:44:14 INFO MemoryStore: MemoryStore cleared
21/02/15 11:44:14 INFO BlockManager: BlockManager stopped
21/02/15 11:44:14 INFO BlockManagerMaster: BlockManagerMaster stopped
21/02/15 11:44:14 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
21/02/15 11:44:14 INFO SparkContext: Successfully stopped SparkContext
21/02/15 11:44:14 INFO ShutdownHookManager: Shutdown hook called
21/02/15 11:44:14 INFO ShutdownHookManager: Deleting directory C:\Users\aaron\AppData\Local\spark\spark-3.0.1-bin-hadoop3.2\tmp\local\spark-1c991ee0-771c-44c3-a9fc-fc592cc13f0f
21/02/15 11:44:14 INFO ShutdownHookManager: Deleting directory C:\Users\aaron\AppData\Local\Temp\spark-a66fe5da-b8ff-44ca-8046-376b260aabc4
21/02/15 11:44:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
21/02/15 11:44:26 INFO SecurityManager: Changing view acls to: aaron
21/02/15 11:44:26 INFO SecurityManager: Changing modify acls to: aaron
21/02/15 11:44:26 INFO SecurityManager: Changing view acls groups to: 
21/02/15 11:44:26 INFO SecurityManager: Changing modify acls groups to: 
21/02/15 11:44:26 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(aaron); groups with view permissions: Set(); users  with modify permissions: Set(aaron); groups with modify permissions: Set()
21/02/15 11:44:27 INFO HiveConf: Found configuration file file:/C:/Users/aaron/AppData/Local/spark/spark-3.0.1-bin-hadoop3.2/conf/hive-site.xml
21/02/15 11:44:27 INFO SparkContext: Running Spark version 3.0.1
21/02/15 11:44:27 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
21/02/15 11:44:27 INFO ResourceUtils: ==============================================================
21/02/15 11:44:27 INFO ResourceUtils: Resources for spark.driver:

21/02/15 11:44:27 INFO ResourceUtils: ==============================================================
21/02/15 11:44:27 INFO SparkContext: Submitted application: sparklyr
21/02/15 11:44:27 INFO SecurityManager: Changing view acls to: aaron
21/02/15 11:44:27 INFO SecurityManager: Changing modify acls to: aaron
21/02/15 11:44:27 INFO SecurityManager: Changing view acls groups to: 
21/02/15 11:44:27 INFO SecurityManager: Changing modify acls groups to: 
21/02/15 11:44:27 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(aaron); groups with view permissions: Set(); users  with modify permissions: Set(aaron); groups with modify permissions: Set()
21/02/15 11:44:27 INFO Utils: Successfully started service 'sparkDriver' on port 53566.
21/02/15 11:44:27 INFO SparkEnv: Registering MapOutputTracker
21/02/15 11:44:27 INFO SparkEnv: Registering BlockManagerMaster
21/02/15 11:44:27 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
21/02/15 11:44:27 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
21/02/15 11:44:27 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
21/02/15 11:44:27 INFO DiskBlockManager: Created local directory at C:\Users\aaron\AppData\Local\spark\spark-3.0.1-bin-hadoop3.2\tmp\local\blockmgr-bae483c3-09f9-47be-a6b0-f0ed2bf10c89
21/02/15 11:44:27 INFO MemoryStore: MemoryStore started with capacity 912.3 MiB
21/02/15 11:44:27 INFO SparkEnv: Registering OutputCommitCoordinator
21/02/15 11:44:27 WARN Utils: The configured local directories are not expected to be URIs; however, got suspicious values [C:/Users/aaron/AppData/Local/spark/spark-3.0.1-bin-hadoop3.2/tmp/local]. Please check your configured local directories.
21/02/15 11:44:28 INFO Utils: Successfully started service 'SparkUI' on port 4040.
21/02/15 11:44:28 INFO SparkUI: Bound SparkUI to 127.0.0.1, and started at http://view-localhost:4040
21/02/15 11:44:28 INFO SparkContext: Added JAR file:/C:/Users/aaron/Documents/R/win-library/4.0/sparklyr/java/sparklyr-3.0-2.12.jar at spark://view-localhost:53566/jars/sparklyr-3.0-2.12.jar with timestamp 1613418268174
21/02/15 11:44:28 INFO Executor: Starting executor ID driver on host view-localhost
21/02/15 11:44:28 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 53613.
21/02/15 11:44:28 INFO NettyBlockTransferService: Server created on view-localhost:53613
21/02/15 11:44:28 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
21/02/15 11:44:28 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, view-localhost, 53613, None)
21/02/15 11:44:28 INFO BlockManagerMasterEndpoint: Registering block manager view-localhost:53613 with 912.3 MiB RAM, BlockManagerId(driver, view-localhost, 53613, None)
21/02/15 11:44:28 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, view-localhost, 53613, None)
21/02/15 11:44:28 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, view-localhost, 53613, None)
21/02/15 11:44:28 INFO SharedState: loading hive config file: file:/C:/Users/aaron/AppData/Local/spark/spark-3.0.1-bin-hadoop3.2/conf/hive-site.xml
21/02/15 11:44:28 INFO SharedState: Setting hive.metastore.warehouse.dir ('C:/Users/aaron/AppData/Local/spark/spark-3.0.1-bin-hadoop3.2/tmp/hive') to the value of spark.sql.warehouse.dir ('C:/Users/aaron/AppData/Local/spark/spark-3.0.1-bin-hadoop3.2/tmp/hive').
21/02/15 11:44:28 INFO SharedState: Warehouse path is 'C:/Users/aaron/AppData/Local/spark/spark-3.0.1-bin-hadoop3.2/tmp/hive'.
21/02/15 11:44:28 WARN SharedState: Not allowing to set spark.sql.warehouse.dir or hive.metastore.warehouse.dir in SparkSession's options, it should be set statically for cross-session usages
21/02/15 11:44:31 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.7 using Spark classes.
21/02/15 11:44:31 INFO HiveConf: Found configuration file file:/C:/Users/aaron/AppData/Local/spark/spark-3.0.1-bin-hadoop3.2/conf/hive-site.xml
21/02/15 11:44:31 INFO SessionState: Created HDFS directory: C:/Users/aaron/AppData/Local/spark/spark-3.0.1-bin-hadoop3.2/tmp/hive/aaron/51fd5b0a-5c3e-4113-a0d1-3473a206fb05
21/02/15 11:44:31 INFO SessionState: Created local directory: C:/Users/aaron/AppData/Local/spark/spark-3.0.1-bin-hadoop3.2/tmp/hive/51fd5b0a-5c3e-4113-a0d1-3473a206fb05
21/02/15 11:44:31 INFO SessionState: Created HDFS directory: C:/Users/aaron/AppData/Local/spark/spark-3.0.1-bin-hadoop3.2/tmp/hive/aaron/51fd5b0a-5c3e-4113-a0d1-3473a206fb05/_tmp_space.db
21/02/15 11:44:31 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.7) is C:/Users/aaron/AppData/Local/spark/spark-3.0.1-bin-hadoop3.2/tmp/hive
21/02/15 11:44:32 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
21/02/15 11:44:32 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
21/02/15 11:44:32 INFO HiveMetaStore: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
21/02/15 11:44:32 INFO ObjectStore: ObjectStore, initialize called
21/02/15 11:44:32 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
21/02/15 11:44:32 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
21/02/15 11:44:33 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
21/02/15 11:44:34 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
21/02/15 11:44:34 INFO ObjectStore: Initialized ObjectStore
21/02/15 11:44:34 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0
21/02/15 11:44:34 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore UNKNOWN@192.168.0.31
21/02/15 11:44:34 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
21/02/15 11:44:34 INFO HiveMetaStore: Added admin role in metastore
21/02/15 11:44:34 INFO HiveMetaStore: Added public role in metastore
21/02/15 11:44:34 INFO HiveMetaStore: No user is added in admin role, since config is empty
21/02/15 11:44:34 INFO HiveMetaStore: 0: get_all_functions
21/02/15 11:44:34 INFO audit: ugi=aaron	ip=unknown-ip-addr	cmd=get_all_functions	
21/02/15 11:44:34 INFO HiveMetaStore: 0: get_database: default
21/02/15 11:44:34 INFO audit: ugi=aaron	ip=unknown-ip-addr	cmd=get_database: default	
21/02/15 11:44:34 INFO HiveMetaStore: 0: get_database: global_temp
21/02/15 11:44:34 INFO audit: ugi=aaron	ip=unknown-ip-addr	cmd=get_database: global_temp	
21/02/15 11:44:34 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
21/02/15 11:44:34 INFO HiveMetaStore: 0: get_database: default
21/02/15 11:44:34 INFO audit: ugi=aaron	ip=unknown-ip-addr	cmd=get_database: default	
21/02/15 11:44:34 INFO HiveMetaStore: 0: get_database: default
21/02/15 11:44:34 INFO audit: ugi=aaron	ip=unknown-ip-addr	cmd=get_database: default	
21/02/15 11:44:34 INFO HiveMetaStore: 0: get_tables: db=default pat=*
21/02/15 11:44:34 INFO audit: ugi=aaron	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
21/02/15 11:44:35 INFO CodeGenerator: Code generated in 170.0121 ms
21/02/15 11:44:35 INFO CodeGenerator: Code generated in 8.5994 ms
21/02/15 11:44:35 INFO SparkContext: Starting job: count at utils.scala:116
21/02/15 11:44:35 INFO DAGScheduler: Registering RDD 2 (count at utils.scala:116) as input to shuffle 0
21/02/15 11:44:35 INFO DAGScheduler: Got job 0 (count at utils.scala:116) with 1 output partitions
21/02/15 11:44:35 INFO DAGScheduler: Final stage: ResultStage 1 (count at utils.scala:116)
21/02/15 11:44:35 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
21/02/15 11:44:35 INFO DAGScheduler: Missing parents: List()
21/02/15 11:44:35 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[5] at count at utils.scala:116), which has no missing parents
21/02/15 11:44:35 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 10.1 KiB, free 912.3 MiB)
21/02/15 11:44:35 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 5.0 KiB, free 912.3 MiB)
21/02/15 11:44:35 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on view-localhost:53613 (size: 5.0 KiB, free: 912.3 MiB)
21/02/15 11:44:35 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1223
21/02/15 11:44:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at count at utils.scala:116) (first 15 tasks are for partitions Vector(0))
21/02/15 11:44:35 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
21/02/15 11:44:35 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 0, view-localhost, executor driver, partition 0, PROCESS_LOCAL, 7325 bytes)
21/02/15 11:44:35 INFO Executor: Running task 0.0 in stage 1.0 (TID 0)
21/02/15 11:44:35 INFO Executor: Fetching spark://view-localhost:53566/jars/sparklyr-3.0-2.12.jar with timestamp 1613418268174
21/02/15 11:44:36 INFO TransportClientFactory: Successfully created connection to view-localhost/127.0.0.1:53566 after 17 ms (0 ms spent in bootstraps)
21/02/15 11:44:36 INFO Utils: Fetching spark://view-localhost:53566/jars/sparklyr-3.0-2.12.jar to C:\Users\aaron\AppData\Local\spark\spark-3.0.1-bin-hadoop3.2\tmp\local\spark-81a2474d-57bc-4186-86c7-3d54159aa0b7\userFiles-1f098950-2470-45ac-839a-a575d789fd6b\fetchFileTemp8276579373166514381.tmp
21/02/15 11:44:36 INFO Executor: Adding file:/C:/Users/aaron/AppData/Local/spark/spark-3.0.1-bin-hadoop3.2/tmp/local/spark-81a2474d-57bc-4186-86c7-3d54159aa0b7/userFiles-1f098950-2470-45ac-839a-a575d789fd6b/sparklyr-3.0-2.12.jar to class loader
21/02/15 11:44:36 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/02/15 11:44:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
21/02/15 11:44:36 INFO Executor: Finished task 0.0 in stage 1.0 (TID 0). 2641 bytes result sent to driver
21/02/15 11:44:36 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 0) in 339 ms on view-localhost (executor driver) (1/1)
21/02/15 11:44:36 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
21/02/15 11:44:36 INFO DAGScheduler: ResultStage 1 (count at utils.scala:116) finished in 0.477 s
21/02/15 11:44:36 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
21/02/15 11:44:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
21/02/15 11:44:36 INFO DAGScheduler: Job 0 finished: count at utils.scala:116, took 0.517665 s
21/02/15 11:44:39 WARN ProcfsMetricsGetter: Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped
21/02/15 11:45:33 INFO SparkContext: Invoking stop() from shutdown hook
21/02/15 11:45:33 INFO SparkUI: Stopped Spark web UI at http://view-localhost:4040
21/02/15 11:45:33 INFO BlockManagerInfo: Removed broadcast_0_piece0 on view-localhost:53613 in memory (size: 5.0 KiB, free: 912.3 MiB)
21/02/15 11:45:33 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
21/02/15 11:45:33 INFO MemoryStore: MemoryStore cleared
21/02/15 11:45:33 INFO BlockManager: BlockManager stopped
21/02/15 11:45:33 INFO BlockManagerMaster: BlockManagerMaster stopped
21/02/15 11:45:33 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
21/02/15 11:45:33 INFO SparkContext: Successfully stopped SparkContext
21/02/15 11:45:33 INFO ShutdownHookManager: Shutdown hook called
21/02/15 11:45:33 INFO ShutdownHookManager: Deleting directory C:\Users\aaron\AppData\Local\Temp\spark-5bbb1711-8a47-4fbc-92b5-a3aab8d79196
21/02/15 11:45:33 INFO ShutdownHookManager: Deleting directory C:\Users\aaron\AppData\Local\spark\spark-3.0.1-bin-hadoop3.2\tmp\local\spark-81a2474d-57bc-4186-86c7-3d54159aa0b7
21/02/15 11:59:53 INFO Master: Started daemon with process name: 12644@DESKTOP-0L62RVG
21/02/15 11:59:53 WARN Shell: Did not find winutils.exe: {}
java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:548)
	at org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:569)
	at org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:78)
	at org.apache.hadoop.conf.Configuration.getBoolean(Configuration.java:1664)
	at org.apache.hadoop.security.SecurityUtil.setConfigurationInternal(SecurityUtil.java:104)
	at org.apache.hadoop.security.SecurityUtil.<clinit>(SecurityUtil.java:88)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:316)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:304)
	at org.apache.hadoop.security.UserGroupInformation.doSubjectLogin(UserGroupInformation.java:1828)
	at org.apache.hadoop.security.UserGroupInformation.createLoginUser(UserGroupInformation.java:710)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:660)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:571)
	at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2412)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2412)
	at org.apache.spark.SecurityManager.<init>(SecurityManager.scala:79)
	at org.apache.spark.deploy.master.Master$.startRpcEnvAndEndpoint(Master.scala:1139)
	at org.apache.spark.deploy.master.Master$.main(Master.scala:1124)
	at org.apache.spark.deploy.master.Master.main(Master.scala)
Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:468)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:439)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:516)
	... 17 more
21/02/15 11:59:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
21/02/15 11:59:53 INFO SecurityManager: Changing view acls to: aaron
21/02/15 11:59:53 INFO SecurityManager: Changing modify acls to: aaron
21/02/15 11:59:53 INFO SecurityManager: Changing view acls groups to: 
21/02/15 11:59:53 INFO SecurityManager: Changing modify acls groups to: 
21/02/15 11:59:53 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(aaron); groups with view permissions: Set(); users  with modify permissions: Set(aaron); groups with modify permissions: Set()
21/02/15 11:59:54 WARN Utils: Service 'sparkMaster' could not bind on port 7077. Attempting port 7078.
21/02/15 11:59:54 INFO Utils: Successfully started service 'sparkMaster' on port 7078.
21/02/15 11:59:54 INFO Master: Starting Spark master at spark://192.168.0.31:7078
21/02/15 11:59:54 INFO Master: Running Spark version 3.0.1
21/02/15 11:59:55 WARN Utils: Service 'MasterUI' could not bind on port 8080. Attempting port 8081.
21/02/15 11:59:55 INFO Utils: Successfully started service 'MasterUI' on port 8081.
21/02/15 11:59:55 INFO MasterWebUI: Bound MasterWebUI to 0.0.0.0, and started at http://host.docker.internal:8081
21/02/15 11:59:55 INFO Master: I have been elected leader! New state: ALIVE
21/02/15 11:59:55 INFO Worker: Started daemon with process name: 12340@DESKTOP-0L62RVG
21/02/15 11:59:55 WARN Shell: Did not find winutils.exe: {}
java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:548)
	at org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:569)
	at org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:78)
	at org.apache.hadoop.conf.Configuration.getBoolean(Configuration.java:1664)
	at org.apache.hadoop.security.SecurityUtil.setConfigurationInternal(SecurityUtil.java:104)
	at org.apache.hadoop.security.SecurityUtil.<clinit>(SecurityUtil.java:88)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:316)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:304)
	at org.apache.hadoop.security.UserGroupInformation.doSubjectLogin(UserGroupInformation.java:1828)
	at org.apache.hadoop.security.UserGroupInformation.createLoginUser(UserGroupInformation.java:710)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:660)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:571)
	at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2412)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2412)
	at org.apache.spark.SecurityManager.<init>(SecurityManager.scala:79)
	at org.apache.spark.deploy.worker.Worker$.startRpcEnvAndEndpoint(Worker.scala:857)
	at org.apache.spark.deploy.worker.Worker$.main(Worker.scala:828)
	at org.apache.spark.deploy.worker.Worker.main(Worker.scala)
Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:468)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:439)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:516)
	... 17 more
21/02/15 11:59:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
21/02/15 11:59:55 INFO SecurityManager: Changing view acls to: aaron
21/02/15 11:59:55 INFO SecurityManager: Changing modify acls to: aaron
21/02/15 11:59:55 INFO SecurityManager: Changing view acls groups to: 
21/02/15 11:59:55 INFO SecurityManager: Changing modify acls groups to: 
21/02/15 11:59:55 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(aaron); groups with view permissions: Set(); users  with modify permissions: Set(aaron); groups with modify permissions: Set()
21/02/15 11:59:56 INFO Utils: Successfully started service 'sparkWorker' on port 54238.
21/02/15 11:59:57 INFO Worker: Starting Spark worker 192.168.0.31:54238 with 12 cores, 14.8 GiB RAM
21/02/15 11:59:57 INFO Worker: Running Spark version 3.0.1
21/02/15 11:59:57 INFO Worker: Spark home: C:\Users\aaron\AppData\Local\spark\SPARK-~1.2\bin\..
21/02/15 11:59:57 INFO ResourceUtils: ==============================================================
21/02/15 11:59:57 INFO ResourceUtils: Resources for spark.worker:

21/02/15 11:59:57 INFO ResourceUtils: ==============================================================
21/02/15 11:59:57 WARN Utils: Service 'WorkerUI' could not bind on port 8081. Attempting port 8082.
21/02/15 11:59:57 WARN Utils: Service 'WorkerUI' could not bind on port 8082. Attempting port 8083.
21/02/15 11:59:57 INFO Utils: Successfully started service 'WorkerUI' on port 8083.
21/02/15 11:59:57 INFO WorkerWebUI: Bound WorkerWebUI to 0.0.0.0, and started at http://host.docker.internal:8083
21/02/15 11:59:57 INFO Worker: Connecting to master 192.168.0.31:7077...
21/02/15 11:59:57 INFO TransportClientFactory: Successfully created connection to /192.168.0.31:7077 after 30 ms (0 ms spent in bootstraps)
21/02/15 11:59:57 INFO Master: Registering worker 192.168.0.31:54238 with 12 cores, 14.8 GiB RAM
21/02/15 11:59:57 INFO Worker: Successfully registered with master spark://192.168.0.31:7077
21/02/15 12:51:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
21/02/15 12:51:25 INFO SecurityManager: Changing view acls to: aaron
21/02/15 12:51:25 INFO SecurityManager: Changing modify acls to: aaron
21/02/15 12:51:25 INFO SecurityManager: Changing view acls groups to: 
21/02/15 12:51:25 INFO SecurityManager: Changing modify acls groups to: 
21/02/15 12:51:25 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(aaron); groups with view permissions: Set(); users  with modify permissions: Set(aaron); groups with modify permissions: Set()
21/02/15 12:51:26 INFO HiveConf: Found configuration file file:/C:/Users/aaron/AppData/Local/spark/spark-3.0.1-bin-hadoop3.2/conf/hive-site.xml
21/02/15 12:51:26 INFO SparkContext: Running Spark version 3.0.1
21/02/15 12:51:26 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
21/02/15 12:51:26 INFO ResourceUtils: ==============================================================
21/02/15 12:51:26 INFO ResourceUtils: Resources for spark.driver:

21/02/15 12:51:26 INFO ResourceUtils: ==============================================================
21/02/15 12:51:26 INFO SparkContext: Submitted application: sparklyr
21/02/15 12:51:26 INFO SecurityManager: Changing view acls to: aaron
21/02/15 12:51:26 INFO SecurityManager: Changing modify acls to: aaron
21/02/15 12:51:26 INFO SecurityManager: Changing view acls groups to: 
21/02/15 12:51:26 INFO SecurityManager: Changing modify acls groups to: 
21/02/15 12:51:26 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(aaron); groups with view permissions: Set(); users  with modify permissions: Set(aaron); groups with modify permissions: Set()
21/02/15 12:51:26 INFO Utils: Successfully started service 'sparkDriver' on port 59220.
21/02/15 12:51:26 INFO SparkEnv: Registering MapOutputTracker
21/02/15 12:51:26 INFO SparkEnv: Registering BlockManagerMaster
21/02/15 12:51:26 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
21/02/15 12:51:26 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
21/02/15 12:51:26 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
21/02/15 12:51:26 INFO DiskBlockManager: Created local directory at C:\Users\aaron\AppData\Local\spark\spark-3.0.1-bin-hadoop3.2\tmp\local\blockmgr-0ae6a999-9efa-4992-a603-992fea572bba
21/02/15 12:51:26 INFO MemoryStore: MemoryStore started with capacity 912.3 MiB
21/02/15 12:51:26 INFO SparkEnv: Registering OutputCommitCoordinator
21/02/15 12:51:26 WARN Utils: The configured local directories are not expected to be URIs; however, got suspicious values [C:/Users/aaron/AppData/Local/spark/spark-3.0.1-bin-hadoop3.2/tmp/local]. Please check your configured local directories.
21/02/15 12:51:27 INFO Utils: Successfully started service 'SparkUI' on port 4040.
21/02/15 12:51:27 INFO SparkUI: Bound SparkUI to 127.0.0.1, and started at http://view-localhost:4040
21/02/15 12:51:27 INFO SparkContext: Added JAR file:/C:/Users/aaron/Documents/R/win-library/4.0/sparklyr/java/sparklyr-3.0-2.12.jar at spark://view-localhost:59220/jars/sparklyr-3.0-2.12.jar with timestamp 1613422287237
21/02/15 12:51:27 INFO Executor: Starting executor ID driver on host view-localhost
21/02/15 12:51:27 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 59267.
21/02/15 12:51:27 INFO NettyBlockTransferService: Server created on view-localhost:59267
21/02/15 12:51:27 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
21/02/15 12:51:27 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, view-localhost, 59267, None)
21/02/15 12:51:27 INFO BlockManagerMasterEndpoint: Registering block manager view-localhost:59267 with 912.3 MiB RAM, BlockManagerId(driver, view-localhost, 59267, None)
21/02/15 12:51:27 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, view-localhost, 59267, None)
21/02/15 12:51:27 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, view-localhost, 59267, None)
21/02/15 12:51:27 INFO SharedState: loading hive config file: file:/C:/Users/aaron/AppData/Local/spark/spark-3.0.1-bin-hadoop3.2/conf/hive-site.xml
21/02/15 12:51:27 INFO SharedState: Setting hive.metastore.warehouse.dir ('C:/Users/aaron/AppData/Local/spark/spark-3.0.1-bin-hadoop3.2/tmp/hive') to the value of spark.sql.warehouse.dir ('C:/Users/aaron/AppData/Local/spark/spark-3.0.1-bin-hadoop3.2/tmp/hive').
21/02/15 12:51:27 INFO SharedState: Warehouse path is 'C:/Users/aaron/AppData/Local/spark/spark-3.0.1-bin-hadoop3.2/tmp/hive'.
21/02/15 12:51:27 WARN SharedState: Not allowing to set spark.sql.warehouse.dir or hive.metastore.warehouse.dir in SparkSession's options, it should be set statically for cross-session usages
21/02/15 12:51:30 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.7 using Spark classes.
21/02/15 12:51:30 INFO HiveConf: Found configuration file file:/C:/Users/aaron/AppData/Local/spark/spark-3.0.1-bin-hadoop3.2/conf/hive-site.xml
21/02/15 12:51:30 INFO SessionState: Created HDFS directory: C:/Users/aaron/AppData/Local/spark/spark-3.0.1-bin-hadoop3.2/tmp/hive/aaron/c7b2e2ee-e911-429f-aa9c-486d7b812460
21/02/15 12:51:30 INFO SessionState: Created local directory: C:/Users/aaron/AppData/Local/spark/spark-3.0.1-bin-hadoop3.2/tmp/hive/c7b2e2ee-e911-429f-aa9c-486d7b812460
21/02/15 12:51:30 INFO SessionState: Created HDFS directory: C:/Users/aaron/AppData/Local/spark/spark-3.0.1-bin-hadoop3.2/tmp/hive/aaron/c7b2e2ee-e911-429f-aa9c-486d7b812460/_tmp_space.db
21/02/15 12:51:30 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.7) is C:/Users/aaron/AppData/Local/spark/spark-3.0.1-bin-hadoop3.2/tmp/hive
21/02/15 12:51:31 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
21/02/15 12:51:31 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
21/02/15 12:51:31 INFO HiveMetaStore: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
21/02/15 12:51:31 INFO ObjectStore: ObjectStore, initialize called
21/02/15 12:51:31 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
21/02/15 12:51:31 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
21/02/15 12:51:32 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
21/02/15 12:51:33 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
21/02/15 12:51:33 INFO ObjectStore: Initialized ObjectStore
21/02/15 12:51:33 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0
21/02/15 12:51:33 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore UNKNOWN@192.168.0.31
21/02/15 12:51:33 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
21/02/15 12:51:33 INFO HiveMetaStore: Added admin role in metastore
21/02/15 12:51:33 INFO HiveMetaStore: Added public role in metastore
21/02/15 12:51:33 INFO HiveMetaStore: No user is added in admin role, since config is empty
21/02/15 12:51:34 INFO HiveMetaStore: 0: get_all_functions
21/02/15 12:51:34 INFO audit: ugi=aaron	ip=unknown-ip-addr	cmd=get_all_functions	
21/02/15 12:51:34 INFO HiveMetaStore: 0: get_database: default
21/02/15 12:51:34 INFO audit: ugi=aaron	ip=unknown-ip-addr	cmd=get_database: default	
21/02/15 12:51:34 INFO HiveMetaStore: 0: get_database: global_temp
21/02/15 12:51:34 INFO audit: ugi=aaron	ip=unknown-ip-addr	cmd=get_database: global_temp	
21/02/15 12:51:34 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
21/02/15 12:51:34 INFO HiveMetaStore: 0: get_database: default
21/02/15 12:51:34 INFO audit: ugi=aaron	ip=unknown-ip-addr	cmd=get_database: default	
21/02/15 12:51:34 INFO HiveMetaStore: 0: get_database: default
21/02/15 12:51:34 INFO audit: ugi=aaron	ip=unknown-ip-addr	cmd=get_database: default	
21/02/15 12:51:34 INFO HiveMetaStore: 0: get_tables: db=default pat=*
21/02/15 12:51:34 INFO audit: ugi=aaron	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
21/02/15 12:51:34 INFO CodeGenerator: Code generated in 174.506999 ms
21/02/15 12:51:35 INFO CodeGenerator: Code generated in 8.869701 ms
21/02/15 12:51:35 INFO SparkContext: Starting job: count at utils.scala:116
21/02/15 12:51:35 INFO DAGScheduler: Registering RDD 2 (count at utils.scala:116) as input to shuffle 0
21/02/15 12:51:35 INFO DAGScheduler: Got job 0 (count at utils.scala:116) with 1 output partitions
21/02/15 12:51:35 INFO DAGScheduler: Final stage: ResultStage 1 (count at utils.scala:116)
21/02/15 12:51:35 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
21/02/15 12:51:35 INFO DAGScheduler: Missing parents: List()
21/02/15 12:51:35 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[5] at count at utils.scala:116), which has no missing parents
21/02/15 12:51:35 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 10.1 KiB, free 912.3 MiB)
21/02/15 12:51:35 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 5.0 KiB, free 912.3 MiB)
21/02/15 12:51:35 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on view-localhost:59267 (size: 5.0 KiB, free: 912.3 MiB)
21/02/15 12:51:35 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1223
21/02/15 12:51:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at count at utils.scala:116) (first 15 tasks are for partitions Vector(0))
21/02/15 12:51:35 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
21/02/15 12:51:35 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 0, view-localhost, executor driver, partition 0, PROCESS_LOCAL, 7325 bytes)
21/02/15 12:51:35 INFO Executor: Running task 0.0 in stage 1.0 (TID 0)
21/02/15 12:51:35 INFO Executor: Fetching spark://view-localhost:59220/jars/sparklyr-3.0-2.12.jar with timestamp 1613422287237
21/02/15 12:51:35 INFO TransportClientFactory: Successfully created connection to view-localhost/127.0.0.1:59220 after 19 ms (0 ms spent in bootstraps)
21/02/15 12:51:35 INFO Utils: Fetching spark://view-localhost:59220/jars/sparklyr-3.0-2.12.jar to C:\Users\aaron\AppData\Local\spark\spark-3.0.1-bin-hadoop3.2\tmp\local\spark-93725ae5-bf6b-455f-b798-011750544076\userFiles-ec29ae49-a385-4ce0-8824-87e141c1f290\fetchFileTemp1889759098686221001.tmp
21/02/15 12:51:35 INFO Executor: Adding file:/C:/Users/aaron/AppData/Local/spark/spark-3.0.1-bin-hadoop3.2/tmp/local/spark-93725ae5-bf6b-455f-b798-011750544076/userFiles-ec29ae49-a385-4ce0-8824-87e141c1f290/sparklyr-3.0-2.12.jar to class loader
21/02/15 12:51:35 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/02/15 12:51:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
21/02/15 12:51:35 INFO Executor: Finished task 0.0 in stage 1.0 (TID 0). 2641 bytes result sent to driver
21/02/15 12:51:35 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 0) in 360 ms on view-localhost (executor driver) (1/1)
21/02/15 12:51:35 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
21/02/15 12:51:35 INFO DAGScheduler: ResultStage 1 (count at utils.scala:116) finished in 0.504 s
21/02/15 12:51:35 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
21/02/15 12:51:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
21/02/15 12:51:35 INFO DAGScheduler: Job 0 finished: count at utils.scala:116, took 0.543305 s
21/02/15 12:51:35 INFO HiveMetaStore: 0: get_database: default
21/02/15 12:51:35 INFO audit: ugi=aaron	ip=unknown-ip-addr	cmd=get_database: default	
21/02/15 12:51:35 INFO HiveMetaStore: 0: get_database: default
21/02/15 12:51:35 INFO audit: ugi=aaron	ip=unknown-ip-addr	cmd=get_database: default	
21/02/15 12:51:35 INFO HiveMetaStore: 0: get_tables: db=default pat=*
21/02/15 12:51:35 INFO audit: ugi=aaron	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
21/02/15 12:51:35 INFO BlockManagerInfo: Removed broadcast_0_piece0 on view-localhost:59267 in memory (size: 5.0 KiB, free: 912.3 MiB)
21/02/15 12:51:35 INFO SparkContext: Starting job: count at utils.scala:116
21/02/15 12:51:35 INFO DAGScheduler: Registering RDD 8 (count at utils.scala:116) as input to shuffle 1
21/02/15 12:51:35 INFO DAGScheduler: Got job 1 (count at utils.scala:116) with 1 output partitions
21/02/15 12:51:35 INFO DAGScheduler: Final stage: ResultStage 3 (count at utils.scala:116)
21/02/15 12:51:35 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
21/02/15 12:51:35 INFO DAGScheduler: Missing parents: List()
21/02/15 12:51:35 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[11] at count at utils.scala:116), which has no missing parents
21/02/15 12:51:35 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 10.1 KiB, free 912.3 MiB)
21/02/15 12:51:35 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.0 KiB, free 912.3 MiB)
21/02/15 12:51:35 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on view-localhost:59267 (size: 5.0 KiB, free: 912.3 MiB)
21/02/15 12:51:35 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1223
21/02/15 12:51:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[11] at count at utils.scala:116) (first 15 tasks are for partitions Vector(0))
21/02/15 12:51:35 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks
21/02/15 12:51:35 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 1, view-localhost, executor driver, partition 0, PROCESS_LOCAL, 7325 bytes)
21/02/15 12:51:35 INFO Executor: Running task 0.0 in stage 3.0 (TID 1)
21/02/15 12:51:35 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
21/02/15 12:51:35 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
21/02/15 12:51:35 INFO Executor: Finished task 0.0 in stage 3.0 (TID 1). 2598 bytes result sent to driver
21/02/15 12:51:35 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 1) in 7 ms on view-localhost (executor driver) (1/1)
21/02/15 12:51:35 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
21/02/15 12:51:35 INFO DAGScheduler: ResultStage 3 (count at utils.scala:116) finished in 0.013 s
21/02/15 12:51:35 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
21/02/15 12:51:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
21/02/15 12:51:35 INFO DAGScheduler: Job 1 finished: count at utils.scala:116, took 0.015478 s
21/02/15 12:51:46 WARN ProcfsMetricsGetter: Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped
21/02/15 12:52:58 INFO SparkContext: Invoking stop() from shutdown hook
21/02/15 12:52:58 INFO SparkUI: Stopped Spark web UI at http://view-localhost:4040
21/02/15 12:52:58 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
21/02/15 12:52:58 INFO MemoryStore: MemoryStore cleared
21/02/15 12:52:58 INFO BlockManager: BlockManager stopped
21/02/15 12:52:58 INFO BlockManagerMaster: BlockManagerMaster stopped
21/02/15 12:52:58 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
21/02/15 12:52:58 INFO SparkContext: Successfully stopped SparkContext
21/02/15 12:52:58 INFO ShutdownHookManager: Shutdown hook called
21/02/15 12:52:58 INFO ShutdownHookManager: Deleting directory C:\Users\aaron\AppData\Local\Temp\spark-d76a12f9-562c-4962-9432-3d6977da9088
21/02/15 12:52:58 INFO ShutdownHookManager: Deleting directory C:\Users\aaron\AppData\Local\spark\spark-3.0.1-bin-hadoop3.2\tmp\local\spark-93725ae5-bf6b-455f-b798-011750544076
21/02/15 12:56:58 INFO Master: Started daemon with process name: 804@DESKTOP-0L62RVG
21/02/15 12:56:59 WARN Shell: Did not find winutils.exe: {}
java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:548)
	at org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:569)
	at org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:78)
	at org.apache.hadoop.conf.Configuration.getBoolean(Configuration.java:1664)
	at org.apache.hadoop.security.SecurityUtil.setConfigurationInternal(SecurityUtil.java:104)
	at org.apache.hadoop.security.SecurityUtil.<clinit>(SecurityUtil.java:88)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:316)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:304)
	at org.apache.hadoop.security.UserGroupInformation.doSubjectLogin(UserGroupInformation.java:1828)
	at org.apache.hadoop.security.UserGroupInformation.createLoginUser(UserGroupInformation.java:710)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:660)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:571)
	at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2412)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2412)
	at org.apache.spark.SecurityManager.<init>(SecurityManager.scala:79)
	at org.apache.spark.deploy.master.Master$.startRpcEnvAndEndpoint(Master.scala:1139)
	at org.apache.spark.deploy.master.Master$.main(Master.scala:1124)
	at org.apache.spark.deploy.master.Master.main(Master.scala)
Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:468)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:439)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:516)
	... 17 more
21/02/15 12:56:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
21/02/15 12:56:59 INFO SecurityManager: Changing view acls to: aaron
21/02/15 12:56:59 INFO SecurityManager: Changing modify acls to: aaron
21/02/15 12:56:59 INFO SecurityManager: Changing view acls groups to: 
21/02/15 12:56:59 INFO SecurityManager: Changing modify acls groups to: 
21/02/15 12:56:59 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(aaron); groups with view permissions: Set(); users  with modify permissions: Set(aaron); groups with modify permissions: Set()
21/02/15 12:57:00 WARN Utils: Service 'sparkMaster' could not bind on port 7077. Attempting port 7078.
21/02/15 12:57:00 WARN Utils: Service 'sparkMaster' could not bind on port 7078. Attempting port 7079.
21/02/15 12:57:00 INFO Utils: Successfully started service 'sparkMaster' on port 7079.
21/02/15 12:57:00 INFO Master: Starting Spark master at spark://192.168.0.31:7079
21/02/15 12:57:00 INFO Master: Running Spark version 3.0.1
21/02/15 12:57:00 WARN Utils: Service 'MasterUI' could not bind on port 8080. Attempting port 8081.
21/02/15 12:57:00 WARN Utils: Service 'MasterUI' could not bind on port 8081. Attempting port 8082.
21/02/15 12:57:00 WARN Utils: Service 'MasterUI' could not bind on port 8082. Attempting port 8083.
21/02/15 12:57:00 WARN Utils: Service 'MasterUI' could not bind on port 8083. Attempting port 8084.
21/02/15 12:57:00 INFO Utils: Successfully started service 'MasterUI' on port 8084.
21/02/15 12:57:00 INFO MasterWebUI: Bound MasterWebUI to 0.0.0.0, and started at http://host.docker.internal:8084
21/02/15 12:57:00 INFO Master: I have been elected leader! New state: ALIVE
