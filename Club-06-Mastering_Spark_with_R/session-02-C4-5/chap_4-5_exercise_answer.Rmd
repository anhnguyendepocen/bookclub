---
title: "Chap_4-5_exercise_answer"
author: "Lily Cheng"
date: "2/8/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

###Step 1. Preparation

## 1.1 Load libraries
```{r}
library(sparklyr)
library(ggplot2)
library(dbplot)
library(dplyr)
```

## 1.2 Make a connection to the local spark cluster.
```{r}
sc <- spark_connect(master = "local", version = "2.3")
```

## 1.3 Load data to the spark cluster. file name: house_price.csv
```{r}
df <- spark_read_csv(sc, "house_price.csv", header = TRUE)
```

### Step 2. EDA

## 2.1 Take a look at the data.
```{r}
glimpse(df)
```

## 2.2 Check missing value for all features.
```{r}
df %>% mutate(age = ifelse(is.na(age), "missing", age),
              store = ifelse(is.na(store), "missing", store))
```

## 2.3 Drop the first column of the data.
```{r}
df1 <- df %>%  
  select(-no)
```

## 2.4 Split data into train and test (size = 0.05) set.
```{r}
data_splits <- sdf_random_split(df1, training = 0.95, testing = 0.05, seed = 42)
df_train <- data_splits$training
df_test <- data_splits$testing
```

## 2.5 Get numerical summaries of age and distance of the training data.
```{r}
sdf_describe(df_train, cols = c("age", "distance"))
```

### Step 3. Supervised Learning

## 3.1 Fit training data with decision tree.
```{r}
lr <- ml_decision_tree(
  df_train, price ~ .
)
```

## 3.2 Make prediction on test data.
```{r}
pred <- ml_predict(lr, df_test)
```

## 3.3 Evaluate the prediction (report r2).
```{r}
ml_regression_evaluator(pred, label_col = "price", metric_name = "r2")
```

### Step 4. Pipelines

## 4.1 Build a transformer for turning all features into vectors.
```{r}
ft_vector_assembler(
    input_cols = c("sold_year", "age", "store", 
                   "distance", "x_coord", 'y_coord'), 
    output_col = "features"
  )
```

## 4.2 Build an estimator that estimates feature mean and standard deviation.
```{r}
ft_standard_scaler(input_col = "features", output_col = "features_scaled", 
                     with_mean = TRUE)
```

## 4.3 Create a pipeline with transformer, estimator, and decision tree.
```{r}
pipeline <- ml_pipeline(sc)%>%
  ft_vector_assembler(
    input_cols = c("sold_year", "age", "store", 
                   "distance", "x_coord", 'y_coord'), 
    output_col = "features"
  )%>%
  ft_standard_scaler(input_col = "features", output_col = "features_scaled", 
                     with_mean = TRUE) %>% 
  ml_decision_tree_regressor(features_col = "features_scaled",
                             label_col = "price"
  )

```

## 4.4 Fit the pipeline model with training data.
```{r}
pipeline_model <- ml_fit(pipeline, df_train)
```

## 4.5 Make prediction on test data using the pipeline model and evaluate the prediction (report r2).
```{r}
pred <- ml_predict(pipeline_model, df_test)
ml_regression_evaluator(pred, label_col = "price", metric_name = "r2")

```

## 4.6 Save pipeline model.
```{r}
model_dir <- file.path("spark_model")
ml_save(pipeline_model, model_dir, overwrite = TRUE)

```

## 4.7 Reload pipeline model.
```{r}
model_reload <- ml_load(sc, model_dir)
```

## 4.8 Extract one estimator from the pipeline.
```{r}
ml_stage(model_reload, "decision_tree_regressor")

```

## 4.9 Disconnect to the spark cluster
```{r}
spark_disconnect(sc)
```

### Step 5. Unsupervised Learning

## 5.1 Make a connection to the local spark cluster.
```{r}
sc <- spark_connect(master = "local", version = "2.3")
```

## 5.2 Load `iris` dataset into spark.
```{r}
iris_tbl <- copy_to(sc, iris, "iris", overwrite = TRUE)

```

## 5.3 Take a look at the data.
```{r}
iris_tbl
```

## 5.4 Check missing value for chosen features (Petal_Length, Petal_Width).
```{r}
iris_tbl %>% mutate(Petal_Width = ifelse(is.na(Petal_Width), "missing", Petal_Width),
              Petal_Length = ifelse(is.na(Petal_Length), "missing", Petal_Length))
```
## 5.4 Check missing value for chosen features (Petal_Length, Petal_Width).
```{r}
iris_tbl %>% mutate(Petal_Width = ifelse(is.na(Petal_Width), "missing", Petal_Width),
              Petal_Length = ifelse(is.na(Petal_Length), "missing", Petal_Length))
```

## 5.5 Fit a model using k-means model with 3 clusters.
```{r}
kmeans_model <- iris_tbl %>%
  ml_kmeans(k = 3, features = c("Petal_Length", "Petal_Width"))

kmeans_model
```

## 5.6 Predict the associated class.
```{r}
predicted <- ml_predict(kmeans_model, iris_tbl) %>%
  collect
table(predicted$Species, predicted$prediction)
```

## 5.7 Plot cluster membership.
```{r}
ml_predict(kmeans_model) %>%
  collect() %>%
  ggplot(aes(Petal_Length, Petal_Width)) +
  geom_point(aes(Petal_Width, Petal_Length, col = factor(prediction + 1)),
             size = 2, alpha = 0.5) + 
  geom_point(data = kmeans_model$centers, aes(Petal_Width, Petal_Length),
             col = scales::muted(c("red", "green", "blue")),
             pch = 'x', size = 12) +
  scale_color_discrete(name = "Predicted Cluster",
                       labels = paste("Cluster", 1:3)) +
  labs(
    x = "Petal Length",
    y = "Petal Width",
    title = "K-Means Clustering",
    subtitle = "Use Spark.ML to predict cluster membership with the iris dataset."
  )
```
## 5.8 Disconnect to the spark cluster.
```{r}
spark_disconnect(sc)
```

