---
title: "Mastering Spark with R"
subtitle: 'Chapters 8-9'
author: "Julien Brun"
institute: "Orange County R Users Group"
date: "2021-02-22"
output:
  xaringan::moon_reader:
    css: [default, metropolis, metropolis-fonts, intro.css]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      
---
<style>
pre {
    display: block;
    font-family: monospace;
    white-space: pre;
    margin: 1em 0px;
    margin-top: 0em;
    margin-right: 0px;
    margin-bottom: 0em;
    margin-left: 0px;
}
</style>
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(comment = "")
```

# Intro, Set up, Analysis

* **Spark**: A unified analytics engine for large-scale data processing.
* **Set up**:  `sc <- spark_connect(master = "local", version = "2.3")`
* **Analysis**: `sparklyr` integrates with many R like `dplyr`, `magrittr`, `broom`, `DBI`, `tibble`, `rlang`, and many others

<br>

* However, you cannot use the standard R models. Spark has a large library of modeling commands `MLlib`. 

---

# Data Handling

Beyond big data and big compute, you can also use Spark to improve data:
.large[
* Velocity
* Variety
* Veracity
]

.large[
Data management shift from normalizing all data into centralized databases and/or data warehouses to embracing variety of data managed as data lake.
]

.large[
**=> You  can use Spark to unify data processing from data lakes, databases, and data warehouses through a single interface that is scalable across all of these solutions** 
]

```{r, out.width="60%", echo=FALSE}
knitr::include_graphics("data-data-lake-1.png")
```
---

# Reading Data

- **Folder as a dataset**: Instead of enumerating each file, simply pass the path containing all the files. Spark assumes that every file in that folder is part of the same dataset.
Note: no data is transferred between machines when distributed files are read
- **Predefine a Schema**: either a small sample of the data or an explicitc definition of the columns type as a named vector

Define the schema:
```{r, include=TRUE}
spec_explicit <- c(x = "character", y = "numeric")
spec_explicit
```


Use it When reading the data in:
```{r, eval=FALSE}
spark_read_csv(sc, "data-csv/", columns = spec_with_r)
```

---

# Data in-Memory or as "virtual table"

When you read data in using Spark with R: 

- Default: data is copied into Spark’s distributed memory

- Spark can alo just "map" the files without copying data into memory 

_=> The advantage of this method is that there is almost no up-front time cost to “reading” the file; the mapping is very fast._

_=> The downside is that running queries that actually extract data will take longer._

This is controlled by the memory argument:

```{r, eval = FALSE}
mapped_csv <- spark_read_csv(sc, "data-csv/", memory = FALSE)
```

Selecting a subset of column and caching it in-memory:

```{r, eval=FALSE}
mapped_csv %>%
  dplyr::select(y) %>%
  dplyr::compute("test")
```

---
# Writing Data

All efforts should be made to have reading, processing, and writing of data  happening within the same Spark session and not via R.

```{r, echo=FALSE}
knitr::include_graphics("data-recommended-approach-1.png")
```

What to do if the target is not within the Spark cluster?

- **Spark transfer**: Spark connects to the remote target location and copies the new data; Good if within same data center or cloud provider.
- **External transfer**: Spark writes qury results as fileas and then in the target location, you would use a separate process to transfer the data into the target location.

---

# Copying Data

`copy_to()` and `collect()` are good for data that fites into memory. Use the Hadoop command-line tool to manage such manipulation on large datasets. 

For the latter, you can use the _callback_ parameter to collect data larger that available memory.

**Generally, you should not need to worry about copying large datasets; instead, you can focus on reading and writing different file formats**


---

# Supported File Formats

Out of the box supported formats: 

```{r, echo=FALSE}
knitr::include_graphics("file-formats.png")
```

---

# CSV

Handling poorly formatted csv files:

- **Permissive**: Inserts NULL values for missing tokens
- **Drop Malformed**: Drops lines that are malformed
- **Fail Fast**: Aborts if it encounters any malformed line

---
Example:

```{r, eval=FALSE}
library(sparklyr)

sc <- spark_connect(master = "local", version = "2.3")

## Creates bad test file
writeLines(c("bad", 1, 2, 3, "broken"), "bad.csv")

spark_read_csv(
  sc,
  "bad3",
  "bad.csv",
  columns = list(foo = "integer"),
  options = list(mode = "PERMISSIVE"))

```

---

# JSON

```{r, eval=FALSE}
writeLines("{'a':1, 'b': {'f1': 2, 'f3': 3}}", "data.json")
simple_json <- spark_read_json(sc, "data.json")
simple_json
```
To extract f1 you would run the following transformation:
```{r, eval=FALSE}
simple_json %>% dplyr::transmute(z = get_json_object(to_json(b), 
                                                     '$.f1'))
```

Unnesting:
```{r, eval =FALSE}
sparklyr.nested::sdf_unnest(simple_json, "b")

spark_disconnect(sc)
```

---

# Apache Parquet

Parquet and ORC store data in columnar format, while AVRO is row-based. 

All of them are binary file formats, which reduces storage space and improves performance, but make them harder ot read.

Spark makes their import pretty straight forward.


---
There is more

There are extension programs, called packages, that let you handle additional formats. 

- You need to load those packages when creating the connection
- `spark_read_source()` and `spark_write_source()` are the generic functions used to read and write thos additional formats

Example:
```{r, eval=FALSE}
sc <- spark_connect(master = "local", version = "2.3", config = list(
  sparklyr.connect.packages = "com.databricks:spark-xml_2.11:0.5.0"))

writeLines("<ROWS><ROW><text>Hello World</text></ROW>", "simple.xml")
spark_read_source(sc, "simple_xml", "simple.xml", "xml")

spark_disconnect(sc)
```

---
# File Systems

**Spark defaults to the file system on which it is currently running**.

- YARN managed cluster: the default file system will be HDFS
- Standalone: filesystem

However, the file system protocol can be changed when reading or writing. You will need the `sparklyr.connect.packages` spark package to configure this: 
 
 - filesystem: `_file://home/user/file.csv_`
 - Databricks: `_dbfs://_`
 - AWS: `_s3a://_`
 - MS Azure: `_wasb://_`
 - Google Cloud: `_gs://_`
 
```{r, eval=FALSE}
Sys.setenv(AWS_ACCESS_KEY_ID = my_key_id)
Sys.setenv(AWS_SECRET_ACCESS_KEY = my_secret_key)

sc <- spark_connect(master = "local", version = "2.3", config = list(
  sparklyr.connect.packages = "org.apache.hadoop:hadoop-aws:2.7.7"))

my_file <- spark_read_csv(sc, "my-file", path = "s3a://my-bucket/my-file.csv")
```

---
# Storage Systems

Data lake and Spark usually go hand-in-hand, with optional access to storage systems like databases and data warehouses. 

Spark can push down computation to the database, a feature known as pushdown _predicates_, which ipmroves performances.

Options:

- **Apache Hive**: data warehouse software natively well integrated in Spark
- **Apache Cassandra**: NoSQL database
- **JBDC**: interface for the programming language Java; when no packages exist for your sepcfic connection

---
# Questions: Data Overview

1. question
2. question



---

class: center, inverse

# Chapter 9: Tuning


---


---

# Questions: Tuning

1. test





