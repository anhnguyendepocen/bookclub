<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Mastering Spark with R</title>
    <meta charset="utf-8" />
    <meta name="author" content="Julien Brun" />
    <meta name="date" content="2021-02-22" />
    <script src="libs/header-attrs-2.6.4/header-attrs.js"></script>
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/metropolis.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/metropolis-fonts.css" rel="stylesheet" />
    <link rel="stylesheet" href="intro.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Mastering Spark with R
## Chapters 8-9
### Julien Brun
### Orange County R Users Group
### 2021-02-22

---

&lt;style&gt;
pre {
    display: block;
    font-family: monospace;
    white-space: pre;
    margin: 1em 0px;
    margin-top: 0em;
    margin-right: 0px;
    margin-bottom: 0em;
    margin-left: 0px;
}
&lt;/style&gt;


# Intro, Set up, Analysis

* **Spark**: A unified analytics engine for large-scale data processing.
* **Set up**:  `sc &lt;- spark_connect(master = "local", version = "2.3")`
* **Analysis**: `sparklyr` integrates with many R like `dplyr`, `magrittr`, `broom`, `DBI`, `tibble`, `rlang`, and many others

&lt;br&gt;

* However, you cannot use the standard R models. Spark has a large library of modeling commands `MLlib`. 

---

# Data Handling

Beyond big data and big compute, you can also use Spark to improve data:
.large[
* Velocity
* Variety
* Veracity
]

.large[
Data management shift from normalizing all data into centralized databases and/or data warehouses to embracing variety of data managed as data lake.
]

.large[
**=&gt; You  can use Spark to unify data processing from data lakes, databases, and data warehouses through a single interface that is scalable across all of these solutions** 
]

&lt;img src="data-data-lake-1.png" width="60%" /&gt;
---

# Reading Data

- **Folder as a dataset**: Instead of enumerating each file, simply pass the path containing all the files. Spark assumes that every file in that folder is part of the same dataset.
Note: no data is transferred between machines when distributed files are read
- **Predefine a Schema**: either a small sample of the data or an explicitc definition of the columns type as a named vector

Define the schema:

```r
spec_explicit &lt;- c(x = "character", y = "numeric")
spec_explicit
```

```
          x           y 
"character"   "numeric" 
```


Use it When reading the data in:

```r
spark_read_csv(sc, "data-csv/", columns = spec_with_r)
```

---

# Data in-Memory or as "virtual table"

When you read data in using Spark with R: 

- Default: data is copied into Spark’s distributed memory

- Spark can alo just "map" the files without copying data into memory 

_=&gt; The advantage of this method is that there is almost no up-front time cost to “reading” the file; the mapping is very fast._

_=&gt; The downside is that running queries that actually extract data will take longer._

This is controlled by the memory argument:


```r
mapped_csv &lt;- spark_read_csv(sc, "data-csv/", memory = FALSE)
```

Selecting a subset of column and caching it in-memory:


```r
mapped_csv %&gt;%
  dplyr::select(y) %&gt;%
  dplyr::compute("test")
```

---
# Writing Data

All efforts should be made to have reading, processing, and writing of data  happening within the same Spark session and not via R.

&lt;img src="data-recommended-approach-1.png" width="2789" /&gt;

What to do if the target is not within the Spark cluster?

- **Spark transfer**: Spark connects to the remote target location and copies the new data; Good if within same data center or cloud provider.
- **External transfer**: Spark writes qury results as fileas and then in the target location, you would use a separate process to transfer the data into the target location.

---

# Copying Data

`copy_to()` and `collect()` are good for data that fites into memory. Use the Hadoop command-line tool to manage such manipulation on large datasets. 

For the latter, you can use the _callback_ parameter to collect data larger that available memory.

**Generally, you should not need to worry about copying large datasets; instead, you can focus on reading and writing different file formats**


---

# Supported File Formats

Out of the box supported formats: 

&lt;img src="file-formats.png" width="2091" /&gt;

---

# CSV

Handling poorly formatted csv files:

- **Permissive**: Inserts NULL values for missing tokens
- **Drop Malformed**: Drops lines that are malformed
- **Fail Fast**: Aborts if it encounters any malformed line

---
Example:


```r
library(sparklyr)

sc &lt;- spark_connect(master = "local", version = "2.3")

## Creates bad test file
writeLines(c("bad", 1, 2, 3, "broken"), "bad.csv")

spark_read_csv(
  sc,
  "bad3",
  "bad.csv",
  columns = list(foo = "integer"),
  options = list(mode = "PERMISSIVE"))
```

---

# JSON


```r
writeLines("{'a':1, 'b': {'f1': 2, 'f3': 3}}", "data.json")
simple_json &lt;- spark_read_json(sc, "data.json")
simple_json
```
To extract f1 you would run the following transformation:

```r
simple_json %&gt;% dplyr::transmute(z = get_json_object(to_json(b), 
                                                     '$.f1'))
```

Unnesting:

```r
sparklyr.nested::sdf_unnest(simple_json, "b")

spark_disconnect(sc)
```

---

# Apache Parquet

Parquet and ORC store data in columnar format, while AVRO is row-based. 

All of them are binary file formats, which reduces storage space and improves performance, but make them harder ot read.

Spark makes their import pretty straight forward.


---
There is more

There are extension programs, called packages, that let you handle additional formats. 

- You need to load those packages when creating the connection
- `spark_read_source()` and `spark_write_source()` are the generic functions used to read and write thos additional formats

Example:

```r
sc &lt;- spark_connect(master = "local", version = "2.3", config = list(
  sparklyr.connect.packages = "com.databricks:spark-xml_2.11:0.5.0"))

writeLines("&lt;ROWS&gt;&lt;ROW&gt;&lt;text&gt;Hello World&lt;/text&gt;&lt;/ROW&gt;", "simple.xml")
spark_read_source(sc, "simple_xml", "simple.xml", "xml")

spark_disconnect(sc)
```

---
# File Systems

**Spark defaults to the file system on which it is currently running**.

- YARN managed cluster: the default file system will be HDFS
- Standalone: filesystem

However, the file system protocol can be changed when reading or writing. You will need the `sparklyr.connect.packages` spark package to configure this: 
 
 - filesystem: `_file://home/user/file.csv_`
 - Databricks: `_dbfs://_`
 - AWS: `_s3a://_`
 - MS Azure: `_wasb://_`
 - Google Cloud: `_gs://_`
 

```r
Sys.setenv(AWS_ACCESS_KEY_ID = my_key_id)
Sys.setenv(AWS_SECRET_ACCESS_KEY = my_secret_key)

sc &lt;- spark_connect(master = "local", version = "2.3", config = list(
  sparklyr.connect.packages = "org.apache.hadoop:hadoop-aws:2.7.7"))

my_file &lt;- spark_read_csv(sc, "my-file", path = "s3a://my-bucket/my-file.csv")
```

---
# Storage Systems

Data lake and Spark usually go hand-in-hand, with optional access to storage systems like databases and data warehouses. 

Spark can push down computation to the database, a feature known as pushdown _predicates_, which ipmroves performances.

Options:

- **Apache Hive**: data warehouse software natively well integrated in Spark
- **Apache Cassandra**: NoSQL database
- **JBDC**: interface for the programming language Java; when no packages exist for your sepcfic connection

---
# Questions: Data Overview

1. question
2. question



---

class: center, inverse

# Chapter 9: Tuning


---


---

# Questions: Tuning

1. test





    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
